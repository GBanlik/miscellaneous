{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Broadly speaking, most recommender systems leverage two types of data:\n",
    "- **Interaction Data** - such as ratings, and browsing behaviors\n",
    "- **Attribution Information** - about each users and items\n",
    "\n",
    "The modeling approach relying on the former data are generally known as *Collaborative Filtering* methods, and approaches using the later are referred to as *Content Base Filtering* method. There is also another category known as *Knowledge-Based* recommender system that is based on explicitly specified user requirements.\n",
    "*Hybrid Systems* are then used to combined the advantages of these approaches to have a robust performing system across a wide variety of applications.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/714/1*dRlaa2hDetPQYLXsvk9I_A.jpeg\">\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Collaborative Filtering Methods**: These types of models use the collaborative power of the ratings provided by multiple users to make recommendations and rely mostly on leveraging either inter-item correlations or inter-user interactions for the prediction process. Intuitively, it relies on an underlying notion that two users who rate items similarly are likely to have comparable preferences for other items. <br><br>\n",
    "There are two types of methods that are commonly used in collaborative filtering:\n",
    "- Memory-based methods also referred to as neighborhood-based collaborative filtering algorithms, where ratings of user-item combinations are predicted based on their neighborhoods. These neighborhoods can be further defined as (1) User Based, and (2) Item Based\n",
    "- Model-based methods use ML techniques are used to learn model parameters within the context of a given optimization framework\n",
    "<br>\n",
    "\n",
    "**Content Based Filtering Methods**: In these types of systems, the descriptive attributes of items/users are used to make recommendations. The term “content” refers to these descriptions. In content-based methods, the ratings and interaction behavior of users are combined with the content information available in the items.\n",
    "<br><br>\n",
    "**Hybrid Methods**: In many cases, a wider variety of inputs is available; in such cases, many opportunities exist for hybridization, where the various aspects from different types of systems are combined to achieve the best of all worlds. The approach is comparable to the conventional ensemble analysis approach, where the power of multiple types of machine learning algorithms is combined to create a more robust model.\n",
    "<br><br>\n",
    "\n",
    "### Implicit vs Explicit Feedback\n",
    "There are two ways in which we can collect data for building recommender systems — *explicit* and *implicit*.\n",
    "<br><br>\n",
    "\n",
    "**Explicit feedback datasets** - The dictionary meaning of explicit is to state clearly and in detail. Explicit feedback data as the name suggests is an exact number given by a user to a product. Some of the examples of explicit feedback are ratings of movies by users on Netflix, ratings of products by users on Amazon. Explicit feedback takes into consideration the input from the user about how they liked or disliked a product. Explicit feedback data are quantifiable.\n",
    "<br><br>\n",
    "\n",
    "**Issues with explicit feedback data**\n",
    "<br>\n",
    "People normally rate a movie or an item on extreme feelings — either they really like the product or when they just hated it. The latter being more prominent. So, chances are the dataset will be largely filled with a lot of positive ratings but very less negative ratings.\n",
    "<br>\n",
    "\n",
    "Explicit feedback is hard to collect as it requires additional input from the users. It needs a separate system to capture this type of data. Then a decision has to be made on whether to go with ratings or like/dislike option to collect the feedback. Each having their merits/demerits.\n",
    "<br>\n",
    "\n",
    "Explicit feedback doesn’t take into consideration the context of when a movie was being watched. For example: a user watched a documentary and really liked it and rated it well. Now, this doesn’t mean the user would like to see a lot many documentaries but with explicit ratings, this becomes difficult to take into consideration. A user may like to binge watch The Office tv series while having dinner and would give it a high rating 4.5/5 but that doesn’t mean that it would watch it at any time of the day.\n",
    "<br>\n",
    "\n",
    "There is another problem with explicit ratings. People rate movies/items differently. Some are lenient with their ratings while others are particular about what ratings they give. User rating bias needs to be taken care of as well.\n",
    "<br>\n",
    "\n",
    "One user will rate a good movie with 3/5 but another may rate a good movie with 4/5, there is then a different in rating methodology per user, and this bias needs to be taken care of.\n",
    "<br><br>\n",
    "\n",
    "**Implicit feedback datasets**\n",
    "<br><br>\n",
    "The dictionary meaning of implicit is suggested though not stated clearly. And that’s exactly what implicit feedback data represents. Implicit feedback doesn’t directly reflect the interest of the user but it acts as a proxy for a user’s interest.\n",
    "\n",
    "<br>\n",
    "\n",
    "Examples of implicit feedback datasets include browsing history, clicks on links, count of the number of times a song is played, the percentage of a webpage you have scrolled — 25%, 50% or 75 % or even the movement of your mouse.\n",
    "\n",
    "<br>\n",
    "\n",
    "The fact that a user browsed an item doesn’t necessarily mean that he liked that item. but if he have browsed this item multiple times that gives us some confidence that he may be interested in that item. Implicit feedback collects information about the user’s actions.\n",
    "\n",
    "<br>\n",
    "\n",
    "Implicit feedback data are found in abundance and are easy to collect. They don’t need any extra input from the user. Once you have the permission of the user to collect their data, your dependence on the user is reduced.\n",
    "\n",
    "<br>\n",
    "\n",
    "Implicit feedback data is characterized by:\n",
    "\n",
    "- No negative preference measured directly -\n",
    "Unlike explicit feedback where a user gives a poor rating for an item, he/she doesn’t like, we do not have a direct way to measure the interest of a user. Repeated action in favor of the item — for eg. listening to Coldplay’s Fix You gives us confidence that the user likes this song and we could recommend a similar song to the user. However, an absence of listening count for a song doesn’t mean that the user does not like the item — may be the user is not even aware of the existence of the song. So, there is no way to measure negative preference directly.\n",
    "- The numerical value of implicit feedback denotes confidence that the user likes the item -\n",
    "For eg., if I’ve listened to Coldplay more number of times than that to Pink Floyd on Youtube, the system would infer with higher confidence about my likeability for Coldplay.\n",
    "- A lot of noisy data to take care of -\n",
    "You need to do a lot of filtering before you actually can get worthwhile data to be modeled upon. Just because I bought an item doesn’t mean I liked the item — may be I bought it for a friend or maybe I didn’t like the item at all after purchasing the item. To handle such issues, we can calculate confidence associated with the preference of the users for items. Read this excellent paper to get an idea of how to incorporate confidence in the feedback data — Collaborative Filtering For Implicit Feedback Datasets.\n",
    "- Missing Values - \n",
    "Explicit feedback datasets are difficult to capture and hence a lot many values are missing and we go about modeling with whatever remaining ratings we have ignoring the missing values. While in the implicit feedback, we assign the missing values as 0 indicating no action from the user — no purchase or not listened to the song.\n",
    "- Evaluation of implicit feedback models require appropriate measures -\n",
    "With explicit feedback — eg. ratings of movies in Netflix data, one can use RMSE (Root mean squared error) as a metric to see how far the predicted ratings are from the observed ratings on a test dataset. With implicit feedback, such a metric is not available. We work with other metrics like decision-support based metrics — precision, recall or rank-based metrics like MRR (mean reciprocal rank), NDCG (normalized discounted cumulative gain), precision@k.\n",
    "\n",
    "\n",
    "### Measures\n",
    "\n",
    "Precision@K and AUC measure different things, and give different perspectives on the quality of the model. In general, they should be correlated, but understanding how they differ may help choose the one that is more important for the application.\n",
    "\n",
    "- Precision@K (or Mean Average Precision @ k, MAP@K) measures the proportion of positive items among the K highest-ranked items. As such, it's very focused on the ranking quality at the top of the list: it doesn't matter how good or bad the rest of the ranking is as long as the first K items are mostly positive. This would be an appropriate metric if you one is only ever going to be showing your users the very top of the list.\n",
    "- AUC measures the quality of the overall ranking. In the binary case, it can be interpreted as the probability that a randomly chosen positive item is ranked higher than a randomly chosen negative item. Consequently, an AUC close to 1.0 will suggest that, by and large, the model's ordering is correct: and this can be true even if none of the first K items are positives. This metric may be more appropriate if one do not exert full control on which results will be presented to the user; it may be that the first K recommended items are not available any more (say, they are out of stock), and there is a need to move further down the ranking. A high AUC score will then give confidence that your ranking is of high quality throughout.\n",
    "\n",
    "## Use Case\n",
    "\n",
    "In our case the dataset is an implicit feedback dataset, representing user's interactions through item purchases. Hence, the model's **score or target**  variable will be the number of purchases, given the number of purchases can serve as a proxy for item preference, often used in implicit feedback datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Horus\\anaconda3\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import time\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "# Utilities\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Colaborative Filtering Model\n",
    "import implicit\n",
    "\n",
    "from implicit.evaluation import precision_at_k, AUC_at_k\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "# Hybrid Model\n",
    "import lightfm\n",
    "from lightfm import LightFM\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.evaluation import auc_score, precision_at_k, recall_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_model(model: LightFM, data: pd.DataFrame, epochs: int = 60, normalize_features: bool = True) -> LightFM:\n",
    "    \n",
    "    dataset = Dataset()\n",
    "\n",
    "    dataset.fit((cac for cac in data.cac.unique())\n",
    "                ,(product for product in data.product_code.unique())\n",
    "               )\n",
    "\n",
    "    features =  ['product_code', 'country_code', 'cost_bin']\n",
    "\n",
    "    for product_feature in features:\n",
    "        dataset.fit_partial(users= (cac for cac in data.cac.unique())\n",
    "                        , items= (product for product in data.product_code.unique())\n",
    "                        , item_features= (feature for feature in data[product_feature].unique())\n",
    "                       )\n",
    "\n",
    "    (interactions, weights) = \\\n",
    "        dataset.build_interactions(((a[0], a[1]) for a in data[['cac', 'product_code']].itertuples(index= False)))\n",
    "\n",
    "    item_features = dataset.build_item_features(((getattr(row, 'product_code'), [getattr(row, product_feature) for features in features if product_feature != 'product_code']) \\\n",
    "             for row in data[features].itertuples()), normalize = normalize_features)\n",
    "\n",
    "    model_loop = model\n",
    "\n",
    "    (train_loop, test_loop) = random_train_test_split(interactions= interactions, test_percentage= 0.2 , random_state = 40)\n",
    "    model_loop.fit(train_loop, item_features= item_features, epochs= epochs, verbose = True)\n",
    "\n",
    "    train_precision_loop = lightfm.evaluation.precision_at_k(model_loop, train_loop, item_features= item_features, k=15).mean()\n",
    "    test_precision_loop = lightfm.evaluation.precision_at_k(model_loop, test_loop, item_features= item_features,k=15).mean()\n",
    "    train_auc_loop = lightfm.evaluation.auc_score(model_loop, train_loop, item_features= item_features).mean()\n",
    "    test_auc_loop = lightfm.evaluation.auc_score(model_loop, test_loop,item_features=item_features).mean()\n",
    "\n",
    "    \n",
    "    num_users, num_items = dataset.interactions_shape()\n",
    "    print('Num users: {}, num_items {}.'.format(num_users, num_items))\n",
    "\n",
    "    print('Precision@15: train %.2f, test %.2f.' % (train_precision_loop, test_precision_loop))\n",
    "    print('AUC@15: train %.2f, test %.2f.' % (train_auc_loop, test_auc_loop))\n",
    "\n",
    "    return model_loop\n",
    "\n",
    "def hybrid_auc_at_k(precision: float, recall: float) -> float:\n",
    "    sorted_idx = recall.argsort()\n",
    "    test_recall = recall[sorted_idx]\n",
    "    test_precision = precision[sorted_idx]\n",
    "\n",
    "    auc = np.round(metrics.auc(test_recall, test_precision), decimals=2)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "\n",
    "def recommend_items_hybrid(model, dataset, user_ids, recommendation_num: int) -> None:\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        # Retrieve the item's IDs\n",
    "        items_map = [item_id for item_id in dataset.mapping()[2].values()]\n",
    "        # Retrieve the product_code for each item ID\n",
    "        items_names = [item_id for item_id in dataset.mapping()[2].keys()]\n",
    "        # Construct a dataframe with product_codes and item ID as index\n",
    "        items = pd.DataFrame(items_names, index = items_map)\n",
    "        items.columns = ['product_code']\n",
    "\n",
    "        # Retrieve the known items\n",
    "        known_items = grouped_data[grouped_data.cac == 'cac_' + str(user_id)]['product_code']\n",
    "        known_item_ids = items[items['product_code'].isin(known_items)].index.tolist()\n",
    "\n",
    "        # Predict items\n",
    "        scores = model.predict([0], np.arange(recommendations_num) ,item_features= item_features)\n",
    "        i_idx = [x for x in np.argsort(-scores)]\n",
    "\n",
    "        # Remove known items\n",
    "        i_idx = [x for x in i_idx if x not in known_item_ids]\n",
    "        top_items = items[~items['product_code'].isin(known_items)].loc[i_idx]\n",
    "        \n",
    "        print('User: ', user_id)\n",
    "        print('Known items sample: ') \n",
    "        print(known_items[: 5].values)\n",
    "\n",
    "        print('Recommended items: ')\n",
    "        print(top_items.values)\n",
    "        \n",
    "        print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_data: pd.DataFrame = pd.read_csv('data_clean_recommendation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Group purchase quantities together by stock code and item ID\n",
    "* Change any sums that equal zero to one (this can happen if items were returned, but we want to indicate that the user actually purchased the item instead of assuming no interaction between the user and the item ever took place)\n",
    "* Only include customers with a positive purchase total to eliminate possible errors\n",
    "* Set up our sparse ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Horus\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "grouped_data: pd.DataFrame = clean_data.drop(['month_code', 'log_date', 'is_peak_period'], axis = 1)\n",
    "grouped_data = grouped_data.groupby(grouped_data.columns.drop('volume_primary_units').tolist()).sum().reset_index()\n",
    "\n",
    "# Replace the zero-sum to a unitary value\n",
    "grouped_data.volume_primary_units.loc[grouped_data.volume_primary_units == 0] = 1\n",
    "\n",
    "# Drop negative purchase quantities\n",
    "grouped_data = grouped_data[grouped_data.volume_primary_units > 0]\n",
    "\n",
    "grouped_data.sample(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, *profit_bin* and *cost_bin* represent the same proportion hence, always reflect the same bin value. Drop *profit_bin* as from a business point of view, its easier to explain the model's recommendation based on price similiarity - which the user perceives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.drop('profit_bin', axis = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of representing an explicit rating, the purchase quantity can represent a “confidence” in terms of how strong the interaction was. Items with a larger number of purchases by a customer can carry more weight in our ratings matrix of purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers with low purchase history 1017\n",
      "Percentage of total customers:  12.555555555555555\n"
     ]
    }
   ],
   "source": [
    "cold_start_users: pd.DataFrame = grouped_data[['cac', 'volume_primary_units']].groupby('cac').sum().reset_index()\n",
    "cold_start_users_count: int = cold_start_users[cold_start_users.volume_primary_units < 10].count().values[0]\n",
    "    \n",
    "print('Number of customers with low purchase history', cold_start_users_count)\n",
    "print('Percentage of total customers: ', (cold_start_users_count / cold_start_users.count().values[0]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "657 customers, or 9% of the transaction customer base have few interactions hence, represent a cold-start problem. Several methods could be used to tackle this problem: use useritem collaborative filtering when no cold-start is detected, and item-item content-based for cold-start situations. However, a hybrid method tends to yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cac</th>\n",
       "      <th>product_code</th>\n",
       "      <th>volume_primary_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2260541</th>\n",
       "      <td>cac_8277</td>\n",
       "      <td>product_code_9918</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937868</th>\n",
       "      <td>cac_6279</td>\n",
       "      <td>product_code_8501</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149111</th>\n",
       "      <td>cac_688</td>\n",
       "      <td>product_code_4456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539433</th>\n",
       "      <td>cac_3931</td>\n",
       "      <td>product_code_478</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966826</th>\n",
       "      <td>cac_6594</td>\n",
       "      <td>product_code_11962</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122810</th>\n",
       "      <td>cac_7569</td>\n",
       "      <td>product_code_3978</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469681</th>\n",
       "      <td>cac_3627</td>\n",
       "      <td>product_code_564</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496839</th>\n",
       "      <td>cac_2442</td>\n",
       "      <td>product_code_8454</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954285</th>\n",
       "      <td>cac_6503</td>\n",
       "      <td>product_code_8458</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757003</th>\n",
       "      <td>cac_494</td>\n",
       "      <td>product_code_8900</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cac        product_code  volume_primary_units\n",
       "2260541  cac_8277   product_code_9918                  15.0\n",
       "1937868  cac_6279   product_code_8501                 100.0\n",
       "1149111   cac_688   product_code_4456                   1.0\n",
       "539433   cac_3931    product_code_478                   1.0\n",
       "1966826  cac_6594  product_code_11962                  36.0\n",
       "2122810  cac_7569   product_code_3978                 144.0\n",
       "469681   cac_3627    product_code_564                   1.0\n",
       "1496839  cac_2442   product_code_8454                 180.0\n",
       "1954285  cac_6503   product_code_8458                  18.0\n",
       "1757003   cac_494   product_code_8900                   6.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions: pd.DataFrame = grouped_data[['cac', 'product_code', 'volume_primary_units']]\n",
    "interactions.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity Model\n",
    "The popularity model has been developed in a separate notebook given its reliance on Unix-only libraries. Specifically, the notebook **\"Recommendation Engine - Popularity Model\"** contains a WSL-based notebook.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'factors': 20, 'regularization': 0.1}\n",
      "{'factors': 20, 'regularization': 0.15}\n",
      "{'factors': 20, 'regularization': 0.2}\n",
      "{'factors': 20, 'regularization': 0.3}\n",
      "{'factors': 40, 'regularization': 0.1}\n",
      "{'factors': 40, 'regularization': 0.15}\n",
      "{'factors': 40, 'regularization': 0.2}\n",
      "{'factors': 40, 'regularization': 0.3}\n",
      "{'factors': 60, 'regularization': 0.1}\n",
      "{'factors': 60, 'regularization': 0.15}\n",
      "{'factors': 60, 'regularization': 0.2}\n",
      "{'factors': 60, 'regularization': 0.3}\n",
      "{'factors': 80, 'regularization': 0.1}\n",
      "{'factors': 80, 'regularization': 0.15}\n",
      "{'factors': 80, 'regularization': 0.2}\n",
      "{'factors': 80, 'regularization': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# Build the user, product and interaction matrices\n",
    "cf_users = grouped_data[['cac', 'product_code', 'volume_primary_units']].copy()\n",
    "\n",
    "cf_users['product'] = cf_users['product_code'].astype('category')\n",
    "cf_users['user'] = cf_users['cac'].astype('category')\n",
    "\n",
    "cf_users['product'] = cf_users['product'].cat.codes\n",
    "cf_users['user'] = cf_users['user'].cat.codes\n",
    "\n",
    "users = cf_users['user'].values\n",
    "products = cf_users['product'].values\n",
    "interactions = cf_users['volume_primary_units'].values\n",
    "\n",
    "mat = sp.csr_matrix((interactions, (users, products)))\n",
    "train, test = implicit.evaluation.train_test_split(mat)\n",
    "\n",
    "alpha = 15\n",
    "train = train * alpha\n",
    "test = test * alpha\n",
    "\n",
    "param_grid = {'factors': [20, 40, 60, 80], 'regularization': [0.1, 0.15, 0.2, 0.3]}\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in grid:\n",
    "    print(params)\n",
    "    model = implicit.als.AlternatingLeastSquares(**params)\n",
    "    model.fit(train, show_progress= False)\n",
    "    \n",
    "    train_csr = train.T.tocsr()\n",
    "    test_csr = test.T.tocsr()\n",
    "    \n",
    "    metric_at_k_params = {'model': model,\n",
    "                          'train_user_items': train_csr,\n",
    "                          'test_user_items': test_csr,\n",
    "                         'show_progress': False}\n",
    "    \n",
    "    precision_at_1 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=1)\n",
    "    precision_at_5 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=5)\n",
    "    precision_at_10 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=10)\n",
    "    precision_at_15 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=15)\n",
    "    \n",
    "    auc_at_1 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=1)\n",
    "    auc_at_5 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=5)\n",
    "    auc_at_10 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=10)\n",
    "    auc_at_15 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=15)\n",
    "   \n",
    "    mapak_at_1 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=1)\n",
    "    mapak_at_5 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=5)\n",
    "    mapak_at_10 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=10)\n",
    "    mapak_at_15 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=15)\n",
    "\n",
    "    ndcg_at_1 = implicit.evaluation.ndcg_at_k(**metric_at_k_params, K=1)\n",
    "    ndcg_at_5 = implicit.evaluation.ndcg_at_k(**metric_at_k_params, K=5)\n",
    "    ndcg_at_10 = implicit.evaluation.ndcg_at_k(**metric_at_k_params, K=10)\n",
    "    ndcg_at_15 = implicit.evaluation.ndcg_at_k(**metric_at_k_params, K=15)\n",
    " \n",
    "    results.append([params['factors'], params['regularization'], \n",
    "                    auc_at_1,\n",
    "                    auc_at_5,\n",
    "                    auc_at_10,\n",
    "                    auc_at_15,\n",
    "                    precision_at_1,\n",
    "                    precision_at_5,\n",
    "                    precision_at_10,\n",
    "                    precision_at_15,\n",
    "                    mapak_at_1,\n",
    "                    mapak_at_5,\n",
    "                    mapak_at_10,\n",
    "                    mapak_at_15,\n",
    "                    ndcg_at_1,\n",
    "                    ndcg_at_5,\n",
    "                    ndcg_at_10,\n",
    "                    ndcg_at_15\n",
    "                   ]) \n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['Factors', 'Regularization',\n",
    "                                            'AUC@1',\n",
    "                                            'AUC@5',\n",
    "                                            'AUC@10',\n",
    "                                            'AUC@15',\n",
    "                                            'Precision@1',\n",
    "                                            'Precision@5',\n",
    "                                            'Precision@10',\n",
    "                                            'Precision@15',\n",
    "                                            'MAP@1',\n",
    "                                            'MAP@5',\n",
    "                                            'MAP@10',\n",
    "                                            'MAP@15',\n",
    "                                            'NDCG@1',\n",
    "                                            'NDCG@5',\n",
    "                                            'NDCG@10',\n",
    "                                            'NDCG@15',\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factors</th>\n",
       "      <th>Regularization</th>\n",
       "      <th>AUC@1</th>\n",
       "      <th>AUC@5</th>\n",
       "      <th>AUC@10</th>\n",
       "      <th>AUC@15</th>\n",
       "      <th>Precision@1</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Precision@15</th>\n",
       "      <th>MAP@1</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>MAP@15</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>NDCG@15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>80</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.514453</td>\n",
       "      <td>0.544888</td>\n",
       "      <td>0.570208</td>\n",
       "      <td>0.589514</td>\n",
       "      <td>0.155130</td>\n",
       "      <td>0.159182</td>\n",
       "      <td>0.181396</td>\n",
       "      <td>0.202180</td>\n",
       "      <td>0.155130</td>\n",
       "      <td>0.097459</td>\n",
       "      <td>0.090564</td>\n",
       "      <td>0.090395</td>\n",
       "      <td>0.155130</td>\n",
       "      <td>0.147277</td>\n",
       "      <td>0.157111</td>\n",
       "      <td>0.167291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>80</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.514032</td>\n",
       "      <td>0.543925</td>\n",
       "      <td>0.569361</td>\n",
       "      <td>0.589141</td>\n",
       "      <td>0.153019</td>\n",
       "      <td>0.159233</td>\n",
       "      <td>0.180811</td>\n",
       "      <td>0.203106</td>\n",
       "      <td>0.153019</td>\n",
       "      <td>0.096111</td>\n",
       "      <td>0.089295</td>\n",
       "      <td>0.089396</td>\n",
       "      <td>0.153019</td>\n",
       "      <td>0.145959</td>\n",
       "      <td>0.155676</td>\n",
       "      <td>0.166353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.513811</td>\n",
       "      <td>0.542957</td>\n",
       "      <td>0.567138</td>\n",
       "      <td>0.586371</td>\n",
       "      <td>0.152823</td>\n",
       "      <td>0.157493</td>\n",
       "      <td>0.178188</td>\n",
       "      <td>0.199940</td>\n",
       "      <td>0.152823</td>\n",
       "      <td>0.096194</td>\n",
       "      <td>0.088620</td>\n",
       "      <td>0.088267</td>\n",
       "      <td>0.152823</td>\n",
       "      <td>0.145239</td>\n",
       "      <td>0.153573</td>\n",
       "      <td>0.163676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.513718</td>\n",
       "      <td>0.542268</td>\n",
       "      <td>0.566834</td>\n",
       "      <td>0.585995</td>\n",
       "      <td>0.148650</td>\n",
       "      <td>0.155241</td>\n",
       "      <td>0.176861</td>\n",
       "      <td>0.198913</td>\n",
       "      <td>0.148650</td>\n",
       "      <td>0.094141</td>\n",
       "      <td>0.087184</td>\n",
       "      <td>0.087098</td>\n",
       "      <td>0.148650</td>\n",
       "      <td>0.142669</td>\n",
       "      <td>0.151733</td>\n",
       "      <td>0.162055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.511781</td>\n",
       "      <td>0.538587</td>\n",
       "      <td>0.561745</td>\n",
       "      <td>0.580608</td>\n",
       "      <td>0.130928</td>\n",
       "      <td>0.139017</td>\n",
       "      <td>0.161093</td>\n",
       "      <td>0.182898</td>\n",
       "      <td>0.130928</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>0.076096</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>0.130928</td>\n",
       "      <td>0.127298</td>\n",
       "      <td>0.137052</td>\n",
       "      <td>0.147791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.511555</td>\n",
       "      <td>0.536755</td>\n",
       "      <td>0.560056</td>\n",
       "      <td>0.579213</td>\n",
       "      <td>0.130879</td>\n",
       "      <td>0.138978</td>\n",
       "      <td>0.161687</td>\n",
       "      <td>0.183549</td>\n",
       "      <td>0.130879</td>\n",
       "      <td>0.081230</td>\n",
       "      <td>0.075086</td>\n",
       "      <td>0.075431</td>\n",
       "      <td>0.130879</td>\n",
       "      <td>0.126015</td>\n",
       "      <td>0.135741</td>\n",
       "      <td>0.146380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.511451</td>\n",
       "      <td>0.537852</td>\n",
       "      <td>0.560528</td>\n",
       "      <td>0.578598</td>\n",
       "      <td>0.129013</td>\n",
       "      <td>0.138748</td>\n",
       "      <td>0.160599</td>\n",
       "      <td>0.182731</td>\n",
       "      <td>0.129013</td>\n",
       "      <td>0.081372</td>\n",
       "      <td>0.075406</td>\n",
       "      <td>0.075645</td>\n",
       "      <td>0.129013</td>\n",
       "      <td>0.126597</td>\n",
       "      <td>0.135890</td>\n",
       "      <td>0.146148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.511678</td>\n",
       "      <td>0.537612</td>\n",
       "      <td>0.560513</td>\n",
       "      <td>0.578174</td>\n",
       "      <td>0.128964</td>\n",
       "      <td>0.137750</td>\n",
       "      <td>0.160665</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.128964</td>\n",
       "      <td>0.081223</td>\n",
       "      <td>0.075808</td>\n",
       "      <td>0.075967</td>\n",
       "      <td>0.128964</td>\n",
       "      <td>0.125813</td>\n",
       "      <td>0.135973</td>\n",
       "      <td>0.145880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.509611</td>\n",
       "      <td>0.531648</td>\n",
       "      <td>0.551398</td>\n",
       "      <td>0.568567</td>\n",
       "      <td>0.110702</td>\n",
       "      <td>0.115641</td>\n",
       "      <td>0.135876</td>\n",
       "      <td>0.155948</td>\n",
       "      <td>0.110702</td>\n",
       "      <td>0.066421</td>\n",
       "      <td>0.061422</td>\n",
       "      <td>0.061763</td>\n",
       "      <td>0.110702</td>\n",
       "      <td>0.105726</td>\n",
       "      <td>0.114661</td>\n",
       "      <td>0.124607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.509086</td>\n",
       "      <td>0.531355</td>\n",
       "      <td>0.551963</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.106038</td>\n",
       "      <td>0.117521</td>\n",
       "      <td>0.138366</td>\n",
       "      <td>0.159316</td>\n",
       "      <td>0.106038</td>\n",
       "      <td>0.066113</td>\n",
       "      <td>0.061331</td>\n",
       "      <td>0.061777</td>\n",
       "      <td>0.106038</td>\n",
       "      <td>0.105701</td>\n",
       "      <td>0.115061</td>\n",
       "      <td>0.125150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.508496</td>\n",
       "      <td>0.531398</td>\n",
       "      <td>0.551863</td>\n",
       "      <td>0.568680</td>\n",
       "      <td>0.104615</td>\n",
       "      <td>0.116434</td>\n",
       "      <td>0.137344</td>\n",
       "      <td>0.158222</td>\n",
       "      <td>0.104615</td>\n",
       "      <td>0.065552</td>\n",
       "      <td>0.060395</td>\n",
       "      <td>0.060717</td>\n",
       "      <td>0.104615</td>\n",
       "      <td>0.104922</td>\n",
       "      <td>0.114237</td>\n",
       "      <td>0.124109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.508898</td>\n",
       "      <td>0.530912</td>\n",
       "      <td>0.551174</td>\n",
       "      <td>0.568032</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.114080</td>\n",
       "      <td>0.134994</td>\n",
       "      <td>0.156270</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.063899</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>0.060055</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.103012</td>\n",
       "      <td>0.112457</td>\n",
       "      <td>0.122769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.506833</td>\n",
       "      <td>0.523727</td>\n",
       "      <td>0.539898</td>\n",
       "      <td>0.554021</td>\n",
       "      <td>0.088071</td>\n",
       "      <td>0.090473</td>\n",
       "      <td>0.107162</td>\n",
       "      <td>0.124448</td>\n",
       "      <td>0.088071</td>\n",
       "      <td>0.050017</td>\n",
       "      <td>0.045111</td>\n",
       "      <td>0.045123</td>\n",
       "      <td>0.088071</td>\n",
       "      <td>0.082252</td>\n",
       "      <td>0.089148</td>\n",
       "      <td>0.097433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.507009</td>\n",
       "      <td>0.523867</td>\n",
       "      <td>0.540358</td>\n",
       "      <td>0.553973</td>\n",
       "      <td>0.087923</td>\n",
       "      <td>0.092392</td>\n",
       "      <td>0.108927</td>\n",
       "      <td>0.125683</td>\n",
       "      <td>0.087923</td>\n",
       "      <td>0.050851</td>\n",
       "      <td>0.045845</td>\n",
       "      <td>0.045720</td>\n",
       "      <td>0.087923</td>\n",
       "      <td>0.083420</td>\n",
       "      <td>0.090461</td>\n",
       "      <td>0.098370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.506314</td>\n",
       "      <td>0.522804</td>\n",
       "      <td>0.538651</td>\n",
       "      <td>0.552593</td>\n",
       "      <td>0.085371</td>\n",
       "      <td>0.090358</td>\n",
       "      <td>0.107121</td>\n",
       "      <td>0.124227</td>\n",
       "      <td>0.085371</td>\n",
       "      <td>0.048835</td>\n",
       "      <td>0.043958</td>\n",
       "      <td>0.043866</td>\n",
       "      <td>0.085371</td>\n",
       "      <td>0.080879</td>\n",
       "      <td>0.087588</td>\n",
       "      <td>0.095705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.506543</td>\n",
       "      <td>0.522349</td>\n",
       "      <td>0.538220</td>\n",
       "      <td>0.552980</td>\n",
       "      <td>0.085125</td>\n",
       "      <td>0.089987</td>\n",
       "      <td>0.106197</td>\n",
       "      <td>0.124649</td>\n",
       "      <td>0.085125</td>\n",
       "      <td>0.048364</td>\n",
       "      <td>0.043497</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>0.085125</td>\n",
       "      <td>0.080222</td>\n",
       "      <td>0.086848</td>\n",
       "      <td>0.095815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Factors  Regularization     AUC@1     AUC@5    AUC@10    AUC@15  \\\n",
       "12       80            0.10  0.514453  0.544888  0.570208  0.589514   \n",
       "13       80            0.15  0.514032  0.543925  0.569361  0.589141   \n",
       "14       80            0.20  0.513811  0.542957  0.567138  0.586371   \n",
       "15       80            0.30  0.513718  0.542268  0.566834  0.585995   \n",
       "8        60            0.10  0.511781  0.538587  0.561745  0.580608   \n",
       "9        60            0.15  0.511555  0.536755  0.560056  0.579213   \n",
       "11       60            0.30  0.511451  0.537852  0.560528  0.578598   \n",
       "10       60            0.20  0.511678  0.537612  0.560513  0.578174   \n",
       "6        40            0.20  0.509611  0.531648  0.551398  0.568567   \n",
       "4        40            0.10  0.509086  0.531355  0.551963  0.569000   \n",
       "5        40            0.15  0.508496  0.531398  0.551863  0.568680   \n",
       "7        40            0.30  0.508898  0.530912  0.551174  0.568032   \n",
       "0        20            0.10  0.506833  0.523727  0.539898  0.554021   \n",
       "1        20            0.15  0.507009  0.523867  0.540358  0.553973   \n",
       "2        20            0.20  0.506314  0.522804  0.538651  0.552593   \n",
       "3        20            0.30  0.506543  0.522349  0.538220  0.552980   \n",
       "\n",
       "    Precision@1  Precision@5  Precision@10  Precision@15     MAP@1     MAP@5  \\\n",
       "12     0.155130     0.159182      0.181396      0.202180  0.155130  0.097459   \n",
       "13     0.153019     0.159233      0.180811      0.203106  0.153019  0.096111   \n",
       "14     0.152823     0.157493      0.178188      0.199940  0.152823  0.096194   \n",
       "15     0.148650     0.155241      0.176861      0.198913  0.148650  0.094141   \n",
       "8      0.130928     0.139017      0.161093      0.182898  0.130928  0.082130   \n",
       "9      0.130879     0.138978      0.161687      0.183549  0.130879  0.081230   \n",
       "11     0.129013     0.138748      0.160599      0.182731  0.129013  0.081372   \n",
       "10     0.128964     0.137750      0.160665      0.181785  0.128964  0.081223   \n",
       "6      0.110702     0.115641      0.135876      0.155948  0.110702  0.066421   \n",
       "4      0.106038     0.117521      0.138366      0.159316  0.106038  0.066113   \n",
       "5      0.104615     0.116434      0.137344      0.158222  0.104615  0.065552   \n",
       "7      0.103829     0.114080      0.134994      0.156270  0.103829  0.063899   \n",
       "0      0.088071     0.090473      0.107162      0.124448  0.088071  0.050017   \n",
       "1      0.087923     0.092392      0.108927      0.125683  0.087923  0.050851   \n",
       "2      0.085371     0.090358      0.107121      0.124227  0.085371  0.048835   \n",
       "3      0.085125     0.089987      0.106197      0.124649  0.085125  0.048364   \n",
       "\n",
       "      MAP@10    MAP@15    NDCG@1    NDCG@5   NDCG@10   NDCG@15  \n",
       "12  0.090564  0.090395  0.155130  0.147277  0.157111  0.167291  \n",
       "13  0.089295  0.089396  0.153019  0.145959  0.155676  0.166353  \n",
       "14  0.088620  0.088267  0.152823  0.145239  0.153573  0.163676  \n",
       "15  0.087184  0.087098  0.148650  0.142669  0.151733  0.162055  \n",
       "8   0.076096  0.076586  0.130928  0.127298  0.137052  0.147791  \n",
       "9   0.075086  0.075431  0.130879  0.126015  0.135741  0.146380  \n",
       "11  0.075406  0.075645  0.129013  0.126597  0.135890  0.146148  \n",
       "10  0.075808  0.075967  0.128964  0.125813  0.135973  0.145880  \n",
       "6   0.061422  0.061763  0.110702  0.105726  0.114661  0.124607  \n",
       "4   0.061331  0.061777  0.106038  0.105701  0.115061  0.125150  \n",
       "5   0.060395  0.060717  0.104615  0.104922  0.114237  0.124109  \n",
       "7   0.059365  0.060055  0.103829  0.103012  0.112457  0.122769  \n",
       "0   0.045111  0.045123  0.088071  0.082252  0.089148  0.097433  \n",
       "1   0.045845  0.045720  0.087923  0.083420  0.090461  0.098370  \n",
       "2   0.043958  0.043866  0.085371  0.080879  0.087588  0.095705  \n",
       "3   0.043497  0.043805  0.085125  0.080222  0.086848  0.095815  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by=['Precision@1', 'Precision@5'], ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eb6cf8949044649201aff65c932eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = implicit.als.AlternatingLeastSquares(factors= 80, regularization= 0.2)\n",
    "model.fit(cf_train)\n",
    "\n",
    "with open('cf_model.pickle', 'wb') as pickle_out:\n",
    "    pickle.dump(model, pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Model - Profitability\n",
    "\n",
    "Unfortunately *implicit* does not provide a method for directly estimating a user's precision_at_k value. We can however mimic this, by providing the metric's function only information regarding each user, iterating through the entire set.\n",
    "\n",
    "The profit is then estimated using each user's precision_at_k as a multiplier, applied to the user's test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the user, product and interaction matrices\n",
    "cf_users = grouped_data[['cac', 'product_code', 'volume_primary_units', 'invoiced_sales']].copy()\n",
    "\n",
    "cf_users['product'] = cf_users['product_code'].astype('category')\n",
    "cf_users['user'] = cf_users['cac'].astype('category')\n",
    "\n",
    "cf_users['product'] = cf_users['product'].cat.codes\n",
    "cf_users['user'] = cf_users['user'].cat.codes\n",
    "\n",
    "users = cf_users['user'].values\n",
    "products = cf_users['product'].values\n",
    "interactions = cf_users['volume_primary_units'].values\n",
    "\n",
    "mat = sp.csr_matrix((interactions, (users, products)))\n",
    "cf_train, cf_test = implicit.evaluation.train_test_split(mat)\n",
    "\n",
    "alpha = 15\n",
    "cf_train = cf_train * alpha\n",
    "cf_test = cf_test * alpha\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors= 80, regularization= 0.2)\n",
    "model.fit(cf_train)\n",
    "\n",
    "user_metrics: Dict[str, List[float]] = {}\n",
    "user_profitability_at_k: Dict[str, List[float]] = {}\n",
    "\n",
    "# Set the user as index to speed lookups\n",
    "cf_users.set_index('user', inplace= True, drop = False)\n",
    "\n",
    "# Compute the metrics for each user\n",
    "for user in users:\n",
    "    users_data = cf_users.loc[user]\n",
    "    user = users_data['user'].values\n",
    "    products_user = users_data['product'].values\n",
    "    interactions_user = users_data['volume_primary_units'].values\n",
    "    user_sales = users_data['invoiced_sales']\n",
    "\n",
    "    mat = sp.csr_matrix((interactions_user, (user, products_user)))\n",
    "    train, test = implicit.evaluation.train_test_split(mat)\n",
    "    \n",
    "    train_csr = train.T.tocsr()\n",
    "    test_csr = test.T.tocsr()\n",
    "    \n",
    "    metric_at_k_params = {'model': model,\n",
    "                          'train_user_items': train,\n",
    "                          'test_user_items': test,\n",
    "                         'show_progress': False}\n",
    "    \n",
    "    if test.shape[0] > 0:\n",
    "        precision_at_1 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=1)\n",
    "        precision_at_5 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=5)\n",
    "        precision_at_10 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=10)\n",
    "        precision_at_15 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=15)\n",
    "\n",
    "        cac_id: str = str(users_data['cac'].values[0])\n",
    "        user_metrics[cac_id] = [precision_at_1, precision_at_5, precision_at_10, precision_at_15]\n",
    "\n",
    "        # Finally compute the profitability per user, by getting the test set's user value and multiplying it per each metric value\n",
    "\n",
    "        user_sales_test, _, _ = np.split(users_data['invoiced_sales'], [int(0.2 * len(users_data)), int(0.8 * len(users_data))])\n",
    "        user_total_sales: float = user_sales_test.sum()\n",
    "\n",
    "        profitability_at_1 = user_total_sales * precision_at_1\n",
    "        profitability_at_5 = user_total_sales * precision_at_5\n",
    "        profitability_at_10 = user_total_sales * precision_at_10\n",
    "        profitability_at_15 = user_total_sales * precision_at_15\n",
    "\n",
    "        user_profitability_at_k[cac_id] = [profitability_at_1, profitability_at_5, profitability_at_10, profitability_at_15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: Unfortunately, there is no direct way - without recurring to Cython - in *implicit* to compute the per user precision_at_k, although the previous method would appear to work, the interactions matrix is hardcoded into the model instance, hence, the precision_at_k implementation expects all users and products to appear. This causes the precision_at_k model to eventually hit an error, as the per-user train/test split can differ from that of the main model.\n",
    "\n",
    "A much less accurate approach is to simple multiply the precision_at_k for the test set, considering this value as the potential sales. One possible improvement is to compute the potential sales on the shuffled test set multiple times and average said value, to combat a biased analysis influenced solely by the test set's order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1b7b73c3b942248fb95581152e2a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC@1:  0.500029638952232\n",
      "AUC@5:  0.5000050744518649\n",
      "AUC@10:  0.5000246385796115\n",
      "AUC@15:  0.4999867209912631\n",
      "Precision@1:  0.0020161290322580645\n",
      "Precision@5:  0.002431994235272924\n",
      "Precision@10:  0.002735871179551317\n",
      "Precision@15:  0.003000892670604547\n",
      "MAP@1:  0.0020161290322580645\n",
      "MAP@5:  0.0009503808243727597\n",
      "MAP@10:  0.0006315270844000677\n",
      "MAP@15:  0.0005195527230137442\n",
      "Average Profit at 1:  748252.7243044351\n",
      "Average Profit at 5:  352718.02031795174\n",
      "Average Profit at 10:  234380.76324169646\n",
      "Average Profit at 15:  192823.34324575152\n"
     ]
    }
   ],
   "source": [
    "# Build the user, product and interaction matrices\n",
    "cf_users = grouped_data[['cac', 'product_code', 'volume_primary_units']].copy()\n",
    "\n",
    "cf_users['product'] = cf_users['product_code'].astype('category')\n",
    "cf_users['user'] = cf_users['cac'].astype('category')\n",
    "\n",
    "cf_users['product'] = cf_users['product'].cat.codes\n",
    "cf_users['user'] = cf_users['user'].cat.codes\n",
    "\n",
    "users = cf_users['user'].values\n",
    "products = cf_users['product'].values\n",
    "interactions = cf_users['volume_primary_units'].values\n",
    "\n",
    "mat = sp.csr_matrix((interactions, (users, products)))\n",
    "cf_train, cf_test = implicit.evaluation.train_test_split(mat)\n",
    "\n",
    "alpha = 15\n",
    "cf_train = cf_train * alpha\n",
    "cf_test = cf_test * alpha\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors= 80, regularization= 0.2)\n",
    "model.fit(cf_train)\n",
    "\n",
    "metric_at_k_params = {'model': model,\n",
    "                      'train_user_items': cf_train,\n",
    "                      'test_user_items': cf_test,\n",
    "                     'show_progress': False}\n",
    "    \n",
    "auc_at_1 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=1)\n",
    "auc_at_5 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=5)\n",
    "auc_at_10 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=10)\n",
    "auc_at_15 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=15)\n",
    "\n",
    "precision_at_1 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=1)\n",
    "precision_at_5 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=5)\n",
    "precision_at_10 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=10)\n",
    "precision_at_15 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=15)\n",
    "\n",
    "mapak_at_1 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=1)\n",
    "mapak_at_5 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=5)\n",
    "mapak_at_10 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=10)\n",
    "mapak_at_15 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=15)\n",
    "\n",
    "print('AUC@1: ', auc_at_1)\n",
    "print('AUC@5: ', auc_at_5)\n",
    "print('AUC@10: ', auc_at_10)\n",
    "print('AUC@15: ', auc_at_15)\n",
    "\n",
    "print('Precision@1: ', precision_at_1)\n",
    "print('Precision@5: ', precision_at_5)\n",
    "print('Precision@10: ', precision_at_10)\n",
    "print('Precision@15: ', precision_at_15)\n",
    "\n",
    "print('MAP@1: ', mapak_at_1)\n",
    "print('MAP@5: ', mapak_at_5)\n",
    "print('MAP@10: ', mapak_at_10)\n",
    "print('MAP@15: ', mapak_at_15)\n",
    "\n",
    "avg_profit_at_1: float = 0.0\n",
    "avg_profit_at_5: float = 0.0\n",
    "avg_profit_at_10: float = 0.0\n",
    "avg_profit_at_15: float = 0.0\n",
    "    \n",
    "shuffle_count: int = 50\n",
    "\n",
    "# Note: Use MAP@K since its the true average metric\n",
    "for i in range(shuffle_count):\n",
    "    indices = np.arange(cf_test.shape[0]) #gets the number of rows \n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_matrix = cf_test[list(indices)] \n",
    "    \n",
    "    avg_profit_at_1 = avg_profit_at_1 + (shuffled_matrix * mapak_at_1).sum()\n",
    "    avg_profit_at_5 = avg_profit_at_5 + (shuffled_matrix * mapak_at_5).sum()\n",
    "    avg_profit_at_10 = avg_profit_at_10 + (shuffled_matrix * mapak_at_10).sum()\n",
    "    avg_profit_at_15 = avg_profit_at_15 + (shuffled_matrix * mapak_at_15).sum()\n",
    "    \n",
    "avg_profit_at_1 = avg_profit_at_1 / shuffle_count\n",
    "avg_profit_at_5 = avg_profit_at_5 / shuffle_count\n",
    "avg_profit_at_10 = avg_profit_at_10 / shuffle_count\n",
    "avg_profit_at_15 = avg_profit_at_15 / shuffle_count\n",
    "\n",
    "print('Average Profit at 1: ', avg_profit_at_1)\n",
    "print('Average Profit at 5: ', avg_profit_at_5)\n",
    "print('Average Profit at 10: ', avg_profit_at_10)\n",
    "print('Average Profit at 15: ', avg_profit_at_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profitability outcome is greatly influenced by the train test split, and might differ drastically from other model's. Hence, to mitigate such differences, we generate fixed train test splits and use them to measure profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9619fc847ab441888e741e6e394d7171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC@1:  0.49998733428368836\n",
      "AUC@5:  0.4999601532985219\n",
      "AUC@10:  0.49992168998363024\n",
      "AUC@15:  0.4999787868347142\n",
      "Precision@1:  0.0016792611251049538\n",
      "Precision@5:  0.0027486144279727843\n",
      "Precision@10:  0.0031237797735259665\n",
      "Precision@15:  0.0032656781028688603\n",
      "MAP@1:  0.0016792611251049538\n",
      "MAP@5:  0.0008435488385110551\n",
      "MAP@10:  0.000556834992603254\n",
      "MAP@15:  0.0004413064538245819\n",
      "Average Profit at 1:  592829.5053707967\n",
      "Average Profit at 5:  297798.0215312635\n",
      "Average Profit at 10:  196579.44098331145\n",
      "Average Profit at 15:  155794.4043523406\n"
     ]
    }
   ],
   "source": [
    "# Build the user, product and interaction matrices\n",
    "cf_users = grouped_data[['cac', 'product_code', 'volume_primary_units']].copy()\n",
    "\n",
    "cf_users['product'] = cf_users['product_code'].astype('category')\n",
    "cf_users['user'] = cf_users['cac'].astype('category')\n",
    "\n",
    "cf_users['product'] = cf_users['product'].cat.codes\n",
    "cf_users['user'] = cf_users['user'].cat.codes\n",
    "\n",
    "users = cf_users['user'].values\n",
    "products = cf_users['product'].values\n",
    "interactions = cf_users['volume_primary_units'].values\n",
    "\n",
    "mat = sp.csr_matrix((interactions, (users, products)))\n",
    "cf_train, cf_test = implicit.evaluation.train_test_split(mat)\n",
    "\n",
    "alpha = 15\n",
    "cf_train = cf_train * alpha\n",
    "cf_test = cf_test * alpha\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors= 80, regularization= 0.2)\n",
    "model.fit(cf_train)\n",
    "\n",
    "metric_at_k_params = {'model': model,\n",
    "                      'train_user_items': cf_train,\n",
    "                      'test_user_items': cf_test,\n",
    "                     'show_progress': False}\n",
    "    \n",
    "auc_at_1 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=1)\n",
    "auc_at_5 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=5)\n",
    "auc_at_10 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=10)\n",
    "auc_at_15 = implicit.evaluation.AUC_at_k(**metric_at_k_params, K=15)\n",
    "\n",
    "precision_at_1 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=1)\n",
    "precision_at_5 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=5)\n",
    "precision_at_10 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=10)\n",
    "precision_at_15 = implicit.evaluation.precision_at_k(**metric_at_k_params, K=15)\n",
    "\n",
    "mapak_at_1 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=1)\n",
    "mapak_at_5 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=5)\n",
    "mapak_at_10 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=10)\n",
    "mapak_at_15 = implicit.evaluation.mean_average_precision_at_k(**metric_at_k_params, K=15)\n",
    "\n",
    "print('AUC@1: ', auc_at_1)\n",
    "print('AUC@5: ', auc_at_5)\n",
    "print('AUC@10: ', auc_at_10)\n",
    "print('AUC@15: ', auc_at_15)\n",
    "\n",
    "print('Precision@1: ', precision_at_1)\n",
    "print('Precision@5: ', precision_at_5)\n",
    "print('Precision@10: ', precision_at_10)\n",
    "print('Precision@15: ', precision_at_15)\n",
    "\n",
    "print('MAP@1: ', mapak_at_1)\n",
    "print('MAP@5: ', mapak_at_5)\n",
    "print('MAP@10: ', mapak_at_10)\n",
    "print('MAP@15: ', mapak_at_15)\n",
    "\n",
    "avg_profit_at_1: float = 0.0\n",
    "avg_profit_at_5: float = 0.0\n",
    "avg_profit_at_10: float = 0.0\n",
    "avg_profit_at_15: float = 0.0\n",
    "    \n",
    "\n",
    "shuffle_count: int = 50\n",
    "test_split, train_split = np.split(grouped_data['invoiced_sales'], [int(0.2 * len(grouped_data))])\n",
    "\n",
    "for i in range(shuffle_count):   \n",
    "    test_split = test_split.sample(frac=1)\n",
    "    \n",
    "    avg_profit_at_1 = avg_profit_at_1 + (test_split * mapak_at_1).sum()\n",
    "    avg_profit_at_5 = avg_profit_at_5 + (test_split * mapak_at_5).sum()\n",
    "    avg_profit_at_10 = avg_profit_at_10 + (test_split * mapak_at_10).sum()\n",
    "    avg_profit_at_15 = avg_profit_at_15 + (test_split * mapak_at_15).sum()\n",
    "    \n",
    "avg_profit_at_1 = avg_profit_at_1 / shuffle_count\n",
    "avg_profit_at_5 = avg_profit_at_5 / shuffle_count\n",
    "avg_profit_at_10 = avg_profit_at_10 / shuffle_count\n",
    "avg_profit_at_15 = avg_profit_at_15 / shuffle_count\n",
    "\n",
    "print('Average Profit at 1: ', avg_profit_at_1)\n",
    "print('Average Profit at 5: ', avg_profit_at_5)\n",
    "print('Average Profit at 10: ', avg_profit_at_10)\n",
    "print('Average Profit at 15: ', avg_profit_at_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Filtering Model - Conclusion\n",
    "\n",
    "The top performing collaborative filtering model, with **80** components, achieved the following results:\n",
    "\n",
    "| Metric | Value |       \n",
    "| :- |:-------------: |\n",
    "|AUC@1| 0.51 | \n",
    "|AUC@5| 0.54 | \n",
    "|AUC@10| 0.57 | \n",
    "|AUC@15| 0.58| \n",
    "|Precision@1| 0.15 | \n",
    "|Precision@5| 0.16 | \n",
    "|Precision@10| 0.18 | \n",
    "|Precision@15| 0.20 | \n",
    "|MAP@1| 0.15 | \n",
    "|MAP@5| 0.09 | \n",
    "|MAP@10| 0.09 | \n",
    "|MAP@15| 0.09 | \n",
    "\n",
    "Regarding profitability, the collaborative filtering solution presents a potential revenue of:\n",
    "\n",
    "| k | Potential Revenue |\n",
    "| :- |:-------------: |\n",
    "|1| 533 924.45 mu | \n",
    "|5| 765 671.88 mu | \n",
    "|10| 955 948.87 mu | \n",
    "|15| 1 117 218.67 mu | \n",
    "\n",
    "This is a moderately valuable model, with plenty of room for improval. One of its core weaknesses is its inability to include product and user features, relying solely on the purchase history; combined with its cold-start fragility, the model can be severely improved on both aspects. This is where hybrid models come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model\n",
    "\n",
    "Hybrid recommender system is a special type of recommender system that combines both content and collaborative filtering method. Combining collaborative filtering and content-based filtering could be more effective in some cases. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa). Several studies empirically compare the performance of the hybrid with pure collaborative and content-based methods and demonstrate that hybrid methods can provide more accurate recommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Its important to note that throughout this section, all analysis focused on maximizing the **MAP@K** and **AUC**, using 15 as K. The reasoning behind the choice of 15 is an assumption which states that each website can recommend up to 15 items. In addition, by maximizing AUC@15 and AUC we can rest assured that both MAP@K and AUC@k will perform fairly well for lower K values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 8100, num_items 23170.\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "\n",
    "dataset.fit((cac for cac in grouped_data.cac.unique())\n",
    "            ,(product for product in grouped_data.product_code.unique())\n",
    "           )\n",
    "\n",
    "num_users, num_items = dataset.interactions_shape()\n",
    "\n",
    "print('Num users: {}, num_items {}.'.format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_features: List[str] = ['product_code', 'country_code', 'cost_bin', 'group', 'region_group',\n",
    "                                              'company_area', 'product_family']\n",
    "    \n",
    "for product_feature in product_features:\n",
    "    dataset.fit_partial(users= (cac for cac in grouped_data.cac.unique())\n",
    "                    , items= (product for product in grouped_data.product_code.unique())\n",
    "                    , item_features= (feature for feature in grouped_data[product_feature].unique())\n",
    "                   )\n",
    "    \n",
    "(interactions, weights) = \\\n",
    "    dataset.build_interactions(((a[0], a[1]) for a in grouped_data[['cac', 'product_code']].itertuples(index= False)))\n",
    "\n",
    "item_features = dataset.build_item_features(((getattr(row, 'product_code'), [getattr(row, product_feature) for product_feature in product_features if product_feature != 'product_code']) \\\n",
    "         for row in grouped_data[product_features].itertuples()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We started by training a model on the entire dataset using 250 components.\n",
    "\n",
    "As expected, the model showcased a certain degree of overfitting:\n",
    "- Train AUC: 0.94 vs Test AUC: 0.84\n",
    "- Train Precision@15: 0.10 vs Test Precision@15: 0.03\n",
    "\n",
    "The reasoning behind such expectation is the high number of components regarding the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.10, test 0.03.\n",
      "AUC: train 0.94, test 0.84.\n"
     ]
    }
   ],
   "source": [
    "model_250 = LightFM(loss= 'warp',no_components = 250)\n",
    "\n",
    "model_250.fit(train, item_features= item_features, epochs= 20, verbose = True)\n",
    "\n",
    "train_precision_250 = precision_at_k(model_250, train, item_features= item_features, k=15).mean()\n",
    "test_precision_250 = precision_at_k(model_250, test, item_features= item_features, k=15).mean()\n",
    "train_auc_250 = auc_score(model_250, train, item_features= item_features).mean()\n",
    "test_auc_250 = auc_score(model_250, test,item_features=item_features).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision_250, test_precision_250))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc_250, test_auc_250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural next step would be to assert the overfitting's motif as a sole consequence of the high number of components. The same test was repeated decreasing the component number to 150.\n",
    "\n",
    "Yet, the model still showcased a certain degree of overfitting:\n",
    "- Train AUC: 0.93 vs Test AUC: 0.83\n",
    "- Train Precision@15: 0.09 vs Test Precision@15: 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.02.\n",
      "AUC: train 0.93, test 0.83.\n"
     ]
    }
   ],
   "source": [
    "model = LightFM(loss= 'warp',no_components = 150, max_sampled = 5)\n",
    "\n",
    "(train, test) = random_train_test_split(interactions= interactions, test_percentage= 0.2, random_state = 40)\n",
    "model.fit(train, item_features= item_features, epochs= 20, verbose = True)\n",
    "\n",
    "train_precision = precision_at_k(model, train, item_features= item_features, k=15).mean()\n",
    "test_precision = precision_at_k(model, test, item_features= item_features,k=15).mean()\n",
    "train_auc = auc_score(model, train, item_features= item_features).mean()\n",
    "\n",
    "test_auc = auc_score(model, test,item_features=item_features).mean()\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completeness, three extra tests were carried out without performing any type of feature selection: training with 100, 50 and a smaller version with only 25 components, to assert whether or not the overfitting is being aggravated by the high number of components.\n",
    "\n",
    "Complete model with 100 components:\n",
    "- Train AUC: 0.82 vs Test AUC: 0.82\n",
    "- Train MAP@15: 0.11 vs Test MAP@15: 0.03\n",
    "\n",
    "Complete model with 50 components:\n",
    "- Train AUC: 0.82 vs Test AUC: 0.82\n",
    "- Train MAP@15: 0.10 vs Test MAP@15: 0.03\n",
    "\n",
    "Complete model with 25 components:\n",
    "- Train AUC: 0.82 vs Test AUC: 0.82\n",
    "- Train MAP@15: 0.09 vs Test MAP@15: 0.02\n",
    "\n",
    "Complete model with 15 components:\n",
    "- Train AUC: 0.82 vs Test AUC: 0.82\n",
    "- Train MAP@15: 0.09 vs Test MAP@15: 0.03\n",
    "\n",
    "These brief tests suggest that indeed the number of components was vastly exhagerated, but they do not fully explain the overfitting.\n",
    "\n",
    "Nonetheless, the smallest difference seems to be found at lower component levels, suggesting 25 components or similar might be the adequate number of components to be used. However, this cannot be concluded given the entire feature set is still being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.11, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n"
     ]
    }
   ],
   "source": [
    "model = LightFM(loss= 'warp',no_components = 100, max_sampled = 5)\n",
    "\n",
    "model.fit(train, item_features= item_features, epochs= 20, verbose = True)\n",
    "\n",
    "train_precision = precision_at_k(model, train, item_features= item_features, k=15).mean()\n",
    "test_precision = precision_at_k(model, test, item_features= item_features,k=15).mean()\n",
    "train_auc = auc_score(model, train, item_features= item_features).mean()\n",
    "\n",
    "test_auc = auc_score(model, test,item_features=item_features).mean()\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.10, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n"
     ]
    }
   ],
   "source": [
    "model = LightFM(loss= 'warp',no_components = 50, max_sampled = 5)\n",
    "\n",
    "model.fit(train, item_features= item_features, epochs= 20, verbose = True)\n",
    "\n",
    "train_precision = precision_at_k(model, train, item_features= item_features, k=15).mean()\n",
    "test_precision = precision_at_k(model, test, item_features= item_features,k=15).mean()\n",
    "train_auc = auc_score(model, train, item_features= item_features).mean()\n",
    "\n",
    "test_auc = auc_score(model, test,item_features=item_features).mean()\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.02.\n",
      "AUC: train 0.82, test 0.82.\n"
     ]
    }
   ],
   "source": [
    "model = LightFM(loss= 'warp',no_components = 25, max_sampled = 5)\n",
    "\n",
    "model.fit(train, item_features= item_features, epochs= 20, verbose = True)\n",
    "\n",
    "train_precision = precision_at_k(model, train, item_features= item_features, k=15).mean()\n",
    "test_precision = precision_at_k(model, test, item_features= item_features,k=15).mean()\n",
    "train_auc = auc_score(model, train, item_features= item_features).mean()\n",
    "\n",
    "test_auc = auc_score(model, test,item_features=item_features).mean()\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n"
     ]
    }
   ],
   "source": [
    "model = LightFM(loss= 'warp',no_components = 15, max_sampled = 5)\n",
    "\n",
    "model.fit(train, item_features= item_features, epochs= 20, verbose = True)\n",
    "\n",
    "train_precision = precision_at_k(model, train, item_features= item_features, k=15).mean()\n",
    "test_precision = precision_at_k(model, test, item_features= item_features,k=15).mean()\n",
    "train_auc = auc_score(model, train, item_features= item_features).mean()\n",
    "\n",
    "test_auc = auc_score(model, test,item_features=item_features).mean()\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "\n",
    "Whilst the model benefited from reducing the number of components, the model can likely be improved by tuning its parameters and features.\n",
    "\n",
    "We start by asserting the top performing feature combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 8100, num_items 23170.\n",
      "['product_code', 'country_code']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.12, test 0.04.\n",
      "AUC: train 0.91, test 0.87.\n",
      "['product_code', 'country_code', 'cost_bin']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.11, test 0.03.\n",
      "AUC: train 0.90, test 0.86.\n",
      "['product_code', 'country_code', 'cost_bin', 'group']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.03.\n",
      "AUC: train 0.89, test 0.84.\n",
      "['product_code', 'country_code', 'cost_bin', 'group', 'region_group']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.08, test 0.03.\n",
      "AUC: train 0.89, test 0.83.\n",
      "['product_code', 'country_code', 'cost_bin', 'group', 'region_group', 'company_area']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.08, test 0.02.\n",
      "AUC: train 0.89, test 0.81.\n",
      "['product_code', 'country_code', 'cost_bin', 'group', 'region_group', 'company_area', 'product_family']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.07, test 0.02.\n",
      "AUC: train 0.89, test 0.81.\n"
     ]
    }
   ],
   "source": [
    "feature_combinations = [\n",
    "    ['product_code', 'country_code', 'cost_bin', 'group', 'region_group',\n",
    "                                              'company_area', 'product_family'],\n",
    "    ['product_code', 'country_code', 'cost_bin', 'group', 'region_group',\n",
    "                                              'company_area'],\n",
    "    ['product_code', 'country_code', 'cost_bin', 'group', 'region_group'],\n",
    "    ['product_code', 'country_code', 'cost_bin', 'group'],\n",
    "    ['product_code', 'country_code', 'cost_bin'],\n",
    "    ['product_code', 'country_code']\n",
    "    \n",
    "]\n",
    "\n",
    "feature_combinations.reverse()\n",
    "\n",
    "for feature_combo in feature_combinations:\n",
    "    print(feature_combo)\n",
    "    \n",
    "    model_data: pd.DataFrame = grouped_data[feature_combo]\n",
    "    model_loop = LightFM(loss= 'warp', no_components = 25, max_sampled = 5)\n",
    "\n",
    "    train_hybrid_model(model_loop, model_data, epochs= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not very exaustive, the previous cell's feature search shows that using 2 features (**country_code and cost_bin**) yields the highest score. Let us do a param grid search over the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 5, 'learning_rate': 0.05, 'n': 10, 'no_components': 10}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.05, 'n': 10, 'no_components': 15}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.05, 'n': 10, 'no_components': 25}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.08, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.05, 'n': 20, 'no_components': 10}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.10, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.05, 'n': 20, 'no_components': 15}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.05, 'n': 20, 'no_components': 25}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.10, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.1, 'n': 10, 'no_components': 10}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.1, 'n': 10, 'no_components': 15}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.11, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.1, 'n': 10, 'no_components': 25}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.11, test 0.03.\n",
      "AUC: train 0.83, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.1, 'n': 20, 'no_components': 10}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.02.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.1, 'n': 20, 'no_components': 15}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.10, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 5, 'learning_rate': 0.1, 'n': 20, 'no_components': 25}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.10, test 0.02.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 10, 'learning_rate': 0.05, 'n': 10, 'no_components': 10}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.08, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 10, 'learning_rate': 0.05, 'n': 10, 'no_components': 15}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.08, test 0.02.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 10, 'learning_rate': 0.05, 'n': 10, 'no_components': 25}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 10, 'learning_rate': 0.05, 'n': 20, 'no_components': 10}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.08, test 0.02.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 10, 'learning_rate': 0.05, 'n': 20, 'no_components': 15}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Precision: train 0.09, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n",
      "{'k': 10, 'learning_rate': 0.05, 'n': 20, 'no_components': 25}\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {'learning_rate': [0.05, 0.1], 'k': [5, 10], 'n': [10, 20], 'no_components': [10, 15, 25]}\n",
    "param_list: List[Dict[str, int]] = list(ParameterGrid(param_grid))\n",
    "    \n",
    "for params in param_list:\n",
    "    print(params)\n",
    "    \n",
    "    features =  ['product_code', 'country_code', 'cost_bin']\n",
    "    model_data: pd.DataFrame = grouped_data[features]\n",
    "    model_loop = LightFM(loss= 'warp', **params)\n",
    "\n",
    "    train_hybrid_model(model_loop, model_data, epochs= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter tuning had little to no effect on the evaluation metrics. We opted for selecting the combination which yielded the highest test metric, with lowest train metric - thus decreasing overfitting - which corresponds to using:\n",
    "\n",
    "| Parameter | Value |       \n",
    "| :- |:-------------: |\n",
    "|k| 5 | \n",
    "|learning_rate| 0.05 | \n",
    "|n| 20 | \n",
    "|no_components| 25 | \n",
    "\n",
    "on features **country_code** and **cost_bin**.\n",
    "\n",
    "\n",
    "#### Epoch Optimality\n",
    "\n",
    "Having selected the optimal featureset, all that is left is to assert the optimal number of epochs by validating the model's AUC and Precision@15 per epoch.\n",
    "\n",
    "Note that by now its pretty clear that the hybrid model will not perform any better than 0.03 P@15 in the test set. Nonetheless, we decided to complete the analysis to make sure the problems do not arise due to the lack of tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 8100, num_items 23170.\n",
      "Processing epoch:  0\n",
      "Processing epoch:  1\n",
      "Processing epoch:  2\n",
      "Processing epoch:  3\n",
      "Processing epoch:  4\n",
      "Processing epoch:  5\n",
      "Processing epoch:  6\n",
      "Processing epoch:  7\n",
      "Processing epoch:  8\n",
      "Processing epoch:  9\n",
      "Processing epoch:  10\n",
      "Processing epoch:  11\n",
      "Processing epoch:  12\n",
      "Processing epoch:  13\n",
      "Processing epoch:  14\n",
      "Processing epoch:  15\n",
      "Processing epoch:  16\n",
      "Processing epoch:  17\n",
      "Processing epoch:  18\n",
      "Processing epoch:  19\n",
      "Processing epoch:  20\n",
      "Processing epoch:  21\n",
      "Processing epoch:  22\n",
      "Processing epoch:  23\n",
      "Processing epoch:  24\n",
      "Processing epoch:  25\n",
      "Processing epoch:  26\n",
      "Processing epoch:  27\n",
      "Processing epoch:  28\n",
      "Processing epoch:  29\n",
      "Processing epoch:  30\n",
      "Processing epoch:  31\n",
      "Processing epoch:  32\n",
      "Processing epoch:  33\n",
      "Processing epoch:  34\n",
      "Processing epoch:  35\n",
      "Processing epoch:  36\n",
      "Processing epoch:  37\n",
      "Processing epoch:  38\n",
      "Processing epoch:  39\n",
      "Processing epoch:  40\n",
      "Processing epoch:  41\n",
      "Processing epoch:  42\n",
      "Processing epoch:  43\n",
      "Processing epoch:  44\n",
      "Processing epoch:  45\n",
      "Processing epoch:  46\n",
      "Processing epoch:  47\n",
      "Processing epoch:  48\n",
      "Processing epoch:  49\n",
      "Processing epoch:  50\n",
      "Processing epoch:  51\n",
      "Processing epoch:  52\n",
      "Processing epoch:  53\n",
      "Processing epoch:  54\n",
      "Processing epoch:  55\n",
      "Processing epoch:  56\n",
      "Processing epoch:  57\n",
      "Processing epoch:  58\n",
      "Processing epoch:  59\n",
      "Processing epoch:  60\n",
      "Processing epoch:  61\n",
      "Processing epoch:  62\n",
      "Processing epoch:  63\n",
      "Processing epoch:  64\n",
      "Processing epoch:  65\n",
      "Processing epoch:  66\n",
      "Processing epoch:  67\n",
      "Processing epoch:  68\n",
      "Processing epoch:  69\n",
      "Processing epoch:  70\n",
      "Processing epoch:  71\n",
      "Processing epoch:  72\n",
      "Processing epoch:  73\n",
      "Processing epoch:  74\n",
      "Processing epoch:  75\n",
      "Processing epoch:  76\n",
      "Processing epoch:  77\n",
      "Processing epoch:  78\n",
      "Processing epoch:  79\n",
      "Processing epoch:  80\n",
      "Processing epoch:  81\n",
      "Processing epoch:  82\n",
      "Processing epoch:  83\n",
      "Processing epoch:  84\n",
      "Processing epoch:  85\n",
      "Processing epoch:  86\n",
      "Processing epoch:  87\n",
      "Processing epoch:  88\n",
      "Processing epoch:  89\n",
      "Processing epoch:  90\n",
      "Processing epoch:  91\n",
      "Processing epoch:  92\n",
      "Processing epoch:  93\n",
      "Processing epoch:  94\n",
      "Processing epoch:  95\n",
      "Processing epoch:  96\n",
      "Processing epoch:  97\n",
      "Processing epoch:  98\n",
      "Processing epoch:  99\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8dcny80e1hCWsKmIICLWlLqM1q0t2k6xOvWHttVxqfU3Wm3rjFXn1+m02l9tq23p1CnaunSxUNvBVh1+aGudWsWKqCAgoAgKYUnCmpDlJjf38/vjnMAlBLghCYFz38/H4z5yzznfc8/3y/Lmy/d+z/mauyMiItGV1dcVEBGR3qWgFxGJOAW9iEjEKehFRCJOQS8iEnEKehGRiFPQi4hEnIJeBDCz/zGz7WaW12HfdR3KnWNmVSnbZmY3m9kyM2swsyoz+62ZnXQ46y9yIAp6yXhmNgY4C3Dgk108fSZwC3AzMBA4Hvg98PGeq6FI9+T0dQVEjgBXAn8DXgGuAn6bzklmNg64ETjd3RemHHqsx2so0g0KepEg6L9PEPR/M7Nyd69O47zzgaoOIS9yxNHQjWQ0M/s7YDTwuLu/BrwLXJHm6YOATb1VN5GeoqCXTHcV8Ky7bwm3fx3uA0gAuR3K5wKt4futwLBer6FIN2noRjKWmRUAlwHZZrY53J0H9Dezk4F1wJgOp40F3g/fPwfcb2aV7r7oMFRZ5JCoRy+Z7GKgDZgITAlfE4C/Eozb/wa42symhtMojwe+DMwBcPd3gP8EZofTLmNmlm9mM8zs9j5oj0inTM+jl0xlZvOB5e5+a4f9lwE/AioIAv9WYCRQA/wM+K67J8OyRjC18nqC3v524EXgm+6+/DA1ReSAFPQiIhGnoRsRkYhT0IuIRJyCXkQk4hT0IiIRd0TOox88eLCPGTOmr6shInLUeO2117a4e1lnx47IoB8zZgyLFun+ExGRdJnZ+/s7pqEbEZGIU9CLiEScgl5EJOKOyDF6ETl6tba2UlVVRXNzc19XJZLy8/OpqKggN7fjg1X3T0EvIj2qqqqKkpISxowZQ/AoIOkp7s7WrVupqqpi7NixaZ+noRsR6VHNzc0MGjRIId8LzIxBgwZ1+X9LCnoR6XEK+d5zKL+2aQ3dmNk0gtXus4Gfufs9HY4PAB4GjgWagWvcfVl47D2gnuC53wl3r+xyLUVEDpOOT/R1h2CPp7wPyiU9+Omp5cLt9nOTqce9/ZPaCwTvPfxsMxhSkt/jbTpo0JtZNnA/8BGgCnjVzJ5097dSit0JLHb3T5nZCWH581OOn5uyVJuIZLDUgEz6npDzzo7t9bP9fXAOKee0n59055v/+lWGV4zimhtuxN35x8suZuiIEXz7B/eDw7f+7Q6GDB3OP95wI62tCc4+eRyXfuYqvnT713d/9rWf/gS1NdXk5eWRmxvj3747kxNOPAmAC0+fTGFRMVlZWQwcPIRv/fAnDB5Svk87t23dwkcqJ3D7Xd/l05+9evf+08ZX8LdVVbu3//D4r1n+5hvceff3yMnOYv4Tj/Pd7343bJdzzTXX8M///M/d+jVPp0c/FVjt7msAzGwOMB1IDfqJwLcB3H2lmY0xs3J3r+5W7USkV7QlnebWtuCVSNLc2kZTSxvxRBtNLUmaWttoam2jMZ5gV/hqbk2SdKctGbza3yfanJa2JC2JJC1tSa6ckMOa2l1B8CadNneSybBn2x7S3WRmGEEP2LDgZ/h+8qlTeeap33PV9f9EMplk27at1NfXhefB4tcW8m/f+g5FsRz+/Jc/cdy44/nT07/nm9+8m6ys4INyc7J44KFHmfKBU5nzq19w/3f+nd/9YR5mkJ1l/Pf8PzK4rIy7//1r/OZn/8H37vtBeP3gIgY8N/eXTP3Qh/jL//s9/+fWm3fXL8tg4rDS3W1Z1D+fzUUxThrRj/nz5/PDH/6QZ599luHDh9Pc3Mwvf/nLbv96pRP0I4D1KdtVwIc6lFkCXAK8aGZTgdEEq/NUE/xD+6yZOfCAuz/Y2UXM7HqCVXoYNWpUV9ogEinuTjyRpKklDNuWBDubEtQ3t7IrnqCpJQjo4FjwamppoyWRpLUtSbwtSXN4blMY4O1l28O9ta3rYZubbWRnGdlmZGXteZ+dZeTlZhHLziI3O4tksoRkOAyRm51FXpaRbUE4Z3X2sz2oOUAZM7JS3h9Iyccv4HvfuJPjhhSzdOlSTp0ymU2bNjEwp5XCwkLWvLOKi845g1gsxp+efoJ/ufXL/OQnP+H9lUs4/fTTAYhlZzGoOI/y0nw+dt7ZPPDjH1JWkgdAlhkDi/MYWBTjo+efy49+9CNKC/ad6jj3t4/zg+9/nyuuuIItNZsZMWLE7mM52Xu+Hs3Kygr+4TLj29/+Nvfeey/Dhw8HgqmUn//857v8e9VROkHf2a9qxz8l9wAzzWwxsBR4A0iEx850941mNgT4o5mtdPcX9vnA4B+ABwEqKyu17JUcdRJtSRrDYG2IJ2iIt1Efb6W+OUFjS4KmliSNLQnqmxPsbGplZ1Mr9c2tNLa00dDSRn1zK3Xh/q4EcX5uFoWxnCBoc4zcrCwKYtkUxrIpzsthSEkeBbnZFMSyyc8NXznZ5OdmkZ+bTV5O1l7HCsJXfm4WRXk5FOXlUJyXQ3ZWel8CrlixguOGFAPwjaeW89bGukP69dyficNL+frfn7jf48OHDycnJ4d169axYMECTj/9dDZs2MDLL79Mv379mDx5MrFYjKamJp577jkeeOABduzYwezZs3cHfar58+dz8cUXd3qtp59+mpNOOmmf/evXr2fz5s1MnTqVyy67jN/85jd85StfOWjbli1bxqmnnnrQcl2VTtBXEayX2a4C2JhawN3rgKth9xqaa8MX7r4x/FljZk8QDAXtE/QiR4Lm1jZ2NLayvbGFHY2t7Gxq/9lKQzzBrngbu+KtbGtoYcuuFrY3trCrORjaiCeSaV+nJD+HfgW5lOTnUhTLpl9BLiP659OvIEb/wlyK83IoDMO6IJZDaX4OJfl79ueHwV2Qm512AGeSM888kwULFrBgwQK+8pWvsGHDBhYsWEC/fv0444wzgCCkzz33XAoLC7n00ku56667+MEPfkB2djYAn/nMZ2hoaKCtrY3XX399r88/99xzyc7OZvLkydx99937XH/OnDlcdtllAMyYMYNrr732gEHf27OU0gn6V4FxZjYW2ADMAK5ILWBm/YFGd28BrgNecPc6MysCsty9Pnz/UeCbPdoCkf1wd5pa26hvDoY9tu5qobo+TvXOZmp3xdmyK862hha27mphW0Pwampt2+/nmUFRLOjdDiiKMbg4xqiBhZTk54QBnENRXnZKbzoI5pL89tDOoSDsZWdKOB+o592bzjjjDBYsWMDSpUuZNGkSI0eO5L777qO0tJRrrrkGgNmzZ/PSSy/R/kj0rVu38vzzz3PBBRcA8Nhjj3HyySdz++23c+ONNzJ37tzdn//8888zePDg/V5/9uzZVFdX89hjjwGwceNG3nnnHcaNG0dBQQEtLS3EYjEAtm3btvuzTjzxRF577TXOO++8Hv31OGjQu3vCzG4CniGYXvmwuy83sxvC47OACcAvzKyN4Evaa8PTy4Enwn+tcoBfu/v8Hm2BZJzm1jZq6uJs2tnE5rpmNu1sZltDCzsaW9jZ1B7ozdTUxffby87LyWJwcR6DimMMLIoxbkgxA4tiDCiKMaAw6FX3L8ilX2Eu/QtjlObnUBTLCb6skyPemWeeyX333ccxxxxDdnY2AwcOZMeOHSxfvpyf/vSn1NXV8eKLL7J+/Xry8oKx90ceeYTZs2fvDnqA3Nxc7r77bo499lhWrFjBhAkTDnrtVatW0dDQwIYNG3bv+/rXv86cOXP42te+xoc//GF+9atfcc0119DU1MTjjwezbADuuOMObrvtNp5++mmGDh1KPB7ngQce4Oabb+7Wr0da8+jdfR4wr8O+WSnvXwbGdXLeGuDkbtVQMoa7U1sfZ/32RjbuaKa6rpnNYe97664WtuyKU13XzPbG1n3Ozc/Non9BjH4FuQwsivGBUQMYUpLHwKI8SvJzKC3IZUBhLkNL8xlSmk9pfo5u6omwk046iS1btnDFFVfstW/Xrl0MHjyYRx99lPPOO293yANMnz6d2267jXg8vtdnFRQUcOutt3Lvvffy0EMPHfTas2fP5lOf+tRe+y699FJmzJjB1772NWbOnMkXvvAFfvSjH+HuXHnllZx99tkAXHTRRVRXV3PBBRfg7pjZ7v+BdId1vDngSFBZWelaeCSaGlsSVG1vYv22RtZva2TdtibW7X7fuM/QSX5uFkNK8hlUHGNwcR7lpXmUl+RT3i+fYf3yGdavgGH98inK02ObjhTp9nzl0HX2a2xmr+3vhlT97ZAe1RBPsLpmF+u2NVJd10xNfZyNO5qo2t5E1fZGtuxq2at8QW42owYWMnJgAWceN5jRgwoZNbCQ4f0LGFqaT2mBet4i3aWgl0OyK55gadVOVm2uY+2WBtZsaWBNbQMbdjTtVS4vJ4uh/fKpGFDABRPKGTmwkIoBBYwcWMjIAYUMLo4pyEV6mYJeDsjd2VzXzMrN9bxTXc/b1btYWrWTd2rqSYajfsV5ORxTVsQHxwzgivJRHFtWzNjBReqRZ7D28WXpeYcy3K6gl71U1zXz2vvbeWPddpZtqGPF5jp2pHz5OaQkjwnDSpk2aShTRvXnxOGllBXn6S+17Jafn8/WrVv1qOJe0P48+vz8rj34TEGfwdydtVsaWLh2GwvXbuOVtdt2D73EcrKYMKyUCycNZcKwUsaXlzB+aAn9C2N9XGs50lVUVFBVVUVtbW1fVyWS2leY6goFfQZJtCVZsamehe9t49W121j0/rbdX44OLo7xwTEDuebvxnLq6AFMHFZKLEfLFUjX5ebmdmn1I+l9CvoIc3dWbKrnhXdqeWn1Fl57fzuNLcH0xYoBBZw9rowPjh3I1LEDOWZwkf6bLRJRCvqIqWtu5cV3tvD8yhr+8nYtNfXBzR/HlxfzD6dWUDlmIB8cM4Bh/Qr6uKYicrgo6COgrrmVZ5dX89SSjby0eguJpFOan8NZx5fx4ePLOHtcGUP79fyqNSJydFDQH6WaW9v488oa/rB4A8+vqqUlkaRiQAHXnjWWCyaUc8rI/ns981pEMpeC/ijzZtUOZi9cx1NLNrErnmBwcR5XTB3FJ6cM55SR/TXOLiL7UNAfBeKJNv6weCO/ePk9lm2oIz83i4+fNJxPnTKC048dlDGPvBWRQ6OgP4LVNbcy+5V1PPzSWqrr4owvL+Gb00/k4lNGUJq/79JlIiKdUdAfgWrqmnn4pfd47G/vUx9PcOZxg/jeP5zMWeMGa2hGRLpMQX8E2bijifufX81vF1WRSCa5cNIwvvDhY5hc0b+vqyYiR7G0gt7MpgEzCVaY+pm739Ph+ADgYeBYoBm4xt2XpRzPBhYBG9z9Ez1U98iormvmP59fzeyF63Gcfzh1JF84+xjGDC7q66qJSAQcNOjDkL4f+AjBQuGvmtmT7v5WSrE7gcXu/ikzOyEsf37K8VuAFUBpj9U8AnY2tTLrL+/yyEtrSbQ5n64cyU3nHceI/rqZSUR6Tjo9+qnA6nBZQMxsDjCdYG3YdhOBbwO4+0ozG2Nm5e5ebWYVwMeBbwH7XwY9gyTakjy64D3+48+rqWtuZfrJw/nKR8YzalBhX1dNRCIonaAfAaxP2a4CPtShzBLgEuBFM5sKjAYqgGrgh8BtQMmBLmJm1wPXA4waNSqduh+VVtfUc+tv32TJ+h18+PgyvjrtBCYO1390RKT3pBP0nU3z6Pjk+3uAmWa2GFgKvAEkzOwTQI27v2Zm5xzoIu7+IPAgBGvGplGvo0pb0vnpX9fw/T++TVEsm/+4/BQ+MXmYZtGISK9LJ+irgJEp2xXAxtQC7l4HXA1gQXKtDV8zgE+a2UVAPlBqZr9y98/2QN2PGht3NPHl3yzmlbXbmHbiUO66eBJlJXkHP1FEpAekE/SvAuPMbCywgSC8r0gtYGb9gUZ3bwGuA14Iw/+O8EXYo//nTAv5/35zE3fMfZO2pHPvp0/m0g+MUC9eRA6rgwa9uyfM7CbgGYLplQ+7+3IzuyE8PguYAPzCzNoIvqS9thfrfFRobUvyf+et4JGX3mPKyP7MnDGF0YM0XVJEDj87lIVme1tlZaUvWrSor6txyGrqm7npsTdY+N42rjlzLHdcdAK5epKkiPQiM3vN3Ss7O6Y7Y3vYqs31XPnwK+xsamXmjClMnzKir6skIhlOQd+Dlm3YyeceeoVYThZz//eZmjYpIkcEBX0PeWPddq56eCEl+bn8+vMf0ni8iBwxFPQ9YMn6HXzuoYUMKo7x2HUfomKA7nAVkSOHgr6b3q3dxT8+spCBRTEe/8LplJdqbVYRObJoKkg3bN7ZzJUPLSQ7y/jltVMV8iJyRFKP/hDtbGrdPbtmzvWnaUxeRI5Y6tEfgmTSufXxxazd0sCDnzuVSSP69XWVRET2S0F/CH7yl3f504oa/s/HJ3LGcYP7ujoiIgekoO+iF9/Zwn3PrmL6lOFcefrovq6OiMhBKei7YNPOJm6e8wbHDSnm25ecpIeTichRQUGfJnfnzrlLaWpp4yefPZXCmL7HFpGjg4I+TU8u2cjzq2r5l4+N59iy4r6ujohI2hT0adjW0MI3nnqLk0f256ozxvR1dUREukRBn4a7//st6ppa+c6lJ5GdpXF5ETm6KOgP4sV3tjD39Q380znHcsJQPY1SRI4+aQW9mU0zs1VmttrMbu/k+AAze8LM3jSzhWY2KdyfH24vMbPlZvaNnm5Ab0omnW/NW8GogYX807nH9XV1REQOyUGD3syygfuBC4GJwOVmNrFDsTuBxe4+GbgSmBnujwPnufvJwBRgmpmd1lOV721PvbmRFZvquPWjx5Ofm93X1REROSTp9OinAqvdfU24+PccYHqHMhOB5wDcfSUwxszKPbArLJMbvo68tQs70ZJIct+zbzNhWCl/P3l4X1dHROSQpRP0I4D1KdtV4b5US4BLAMxsKjAaqAi3s81sMVAD/NHdX+nsImZ2vZktMrNFtbW1XWtFL/jNq+tYt62R26aNJ0tfwIrIUSydoO8s5Tr2yu8BBoSB/kXgDSAB4O5t7j6FIPinto/f7/OB7g+6e6W7V5aVlaXdgN7Q2JJg5nOrmTp2IOcc37d1ERHprnRu76wCRqZsVwAbUwu4ex1wNYAFzwVYG75Sy+wws/8BpgHLDr3Kve/nC95ny644D3zuVD3mQESOeun06F8FxpnZWDOLATOAJ1MLmFn/8BjAdcAL7l5nZmVm1j8sUwBcAKzsuer3vJZEkkdeWstZ4wZz6ugBfV0dEZFuO2iP3t0TZnYT8AyQDTzs7svN7Ibw+CxgAvALM2sD3gKuDU8fBvw8nLmTBTzu7k/3Qjt6zJNLNlJTH+d7nz65r6siItIj0noyl7vPA+Z12Dcr5f3LwLhOznsTOKWbdTxs3J2f/XUN48tLOHucnjMvItGgO2NTvLh6Cys313PtWWM1Ni8ikaGgT/HTv66lrCSP6VM0b15EokNBH1q1uZ4X3q7lqtNHk5eju2BFJDoU9KFfvPwe+blZfOZDWh5QRKJFQQ+0JZ1nlm/m/BPKGVAUO/gJIiJHEQU98Nr729myq4Vpk4b2dVVERHqcgh6Yv2wzsewszj1hSF9XRUSkx2V80LsHwzZnjRtMcZ4W/BaR6Mn4oF+6YScbdjRp2EZEIivjg37+ss1kZxkXTCjv66qIiPSKjA56d2f+ss2cdsxAzbYRkcjK6KBfXbOLNVsamDZpWF9XRUSk12R00M9fthkz+NhEDduISHRldNA/v6qGyRX9GVKa39dVERHpNRkb9A3xBG9W7eTMYwf1dVVERHpVWkFvZtPMbJWZrTaz2zs5PsDMnjCzN81sYfu6sGY20syeN7MVZrbczG7p6QYcqlff20Yi6ZyuoBeRiDto0IerQ90PXAhMBC43s4kdit0JLHb3ycCVwMxwfwK41d0nAKcBN3Zybp94ec1WcrONytED+7oqIiK9Kp0e/VRgtbuvcfcWYA4wvUOZicBzAO6+EhhjZuXuvsndXw/31wMrgBE9VvtuePndrZwycgAFMT2SWESiLZ2gHwGsT9muYt+wXgJcAmBmU4HRQEVqATMbQ7Cs4CuHVtWeU9fcyrINOzlNwzYikgHSCfrO1tTzDtv3AAPMbDHwReANgmGb4APMioH/Ar7k7nWdXsTsejNbZGaLamtr06r8oVq4ZhtJh9OPUdCLSPSl8xSvKmBkynYFsDG1QBjeVwNYsNjq2vCFmeUShPxj7j53fxdx9weBBwEqKys7/kPSoxa8u5W8nCxOGdW/Ny8jInJESKdH/yowzszGmlkMmAE8mVrAzPqHxwCuA15w97ow9B8CVrj793uy4t3x8pqtnDp6APm5Gp8Xkeg7aNC7ewK4CXiG4MvUx919uZndYGY3hMUmAMvNbCXB7Jz2aZRnAp8DzjOzxeHroh5vRRdsb2hhxaY6DduISMZI6wHs7j4PmNdh36yU9y8D4zo570U6H+PvM6+s3Qqg+fMikjEy7s7YBe9upTCWzeQKjc+LSGbIuKBfvH4HU0b2J5aTcU0XkQyVUWmXTDpvV9dzwtDSvq6KiMhhk1FBv357I82tScYPLe7rqoiIHDYZFfRvV+8CYFx5SR/XRETk8MmwoK8HYNwQ9ehFJHNkXNCP6F9ASX5uX1dFROSwyaigX7W5nuPL1ZsXkcySMUGfaEuypraB4zU+LyIZJmOC/r2tjbS0JRX0IpJxMibo27+IHT9UQS8imSWjgt4Mji3TGL2IZJaMCvrRAwu1dKCIZJyMCfpVm+t1o5SIZKSMCPp4oo33tjYyXkEvIhkoI4J+TW0DbUnneH0RKyIZKK2gN7NpZrbKzFab2e2dHB9gZk+Y2ZtmttDMJqUce9jMasxsWU9WvCvaZ9zoZikRyUQHDXozywbuJ1gicCJwuZlN7FDsTmCxu08GrgRmphx7FJjWI7U9RG9X15OTZRwzWEEvIpknnR79VGC1u69x9xZgDjC9Q5mJwHMA7r4SGGNm5eH2C8C2nqty163avIsxg4u02IiIZKR0km8EsD5luyrcl2oJcAmAmU0FRgMVXamImV1vZovMbFFtbW1XTj2o97Y2cGxZUY9+pojI0SKdoO9scW/vsH0PMMDMFgNfBN4AEl2piLs/6O6V7l5ZVlbWlVMPqrqumaGl+T36mSIiR4ucNMpUASNTtiuAjakF3L0OuBrAzAxYG776XFNLG/XNCYYo6EUkQ6XTo38VGGdmY80sBswAnkwtYGb9w2MA1wEvhOHf52rqmwEoV9CLSIY6aNC7ewK4CXgGWAE87u7LzewGM7shLDYBWG5mKwlm59zSfr6ZzQZeBsabWZWZXdvTjTiQ6ro4AOWleYfzsiIiR4x0hm5w93nAvA77ZqW8fxkYt59zL+9OBburuk49ehHJbJGfb9ge9ENK1KMXkcwU+aCvrY8Ty8miX4HWiRWRzBT5oK+ua6a8NI9gMpCISObJgKCPU16i8XkRyVzRD/r6ZoZoxo2IZLDIB31tXZwh6tGLSAaLdNA3xBPUxxOaWikiGS3SQV9Tr5ulREQiHfR75tCrRy8imSvSQa8evYhI1IO+vUevMXoRyWCRDvrqumbyc7MozU/rkT4iIpEU8aCPU16ar7tiRSSjRTroa+qb9TAzEcl40Q76urjG50Uk40U66KvrmvWcGxHJeGkFvZlNM7NVZrbazG7v5PgAM3vCzN40s4VmNindc3vLrniChpY2Ta0UkYx30KA3s2zgfoIlAicCl5vZxA7F7gQWu/tk4EpgZhfO7RW7b5ZS0ItIhkunRz8VWO3ua9y9BZgDTO9QZiLwHIC7rwTGmFl5muf2ipr2tWI1dCMiGS6doB8BrE/Zrgr3pVoCXAJgZlOB0UBFmucSnne9mS0ys0W1tbXp1f4Aaup1s5SICKQX9J1NQvcO2/cAA8xsMfBF4A0gkea5wU73B9290t0ry8rK0qjWge1ZFFxDNyKS2dK5ZbQKGJmyXQFsTC3g7nXA1QAW3J20NnwVHuzc3lJdF6cgN5viPN0VKyKZLZ0e/avAODMba2YxYAbwZGoBM+sfHgO4DnghDP+DnttbaurjWitWRIQ0evTunjCzm4BngGzgYXdfbmY3hMdnAROAX5hZG/AWcO2Bzu2dpuytuq5Z4/MiIqQ3dIO7zwPmddg3K+X9y8C4dM89HGrqmpk0ot/hvqyIyBEnsnfG1jcn6F+Y29fVEBHpc5EN+oaWBEUxfRErIhLJoG9LOs2tSQoV9CIi0Qz6xpYEAEV52X1cExGRvhfRoG8DUI9eRISIBn1DPOjRF8bUoxcRiWTQ7+nRK+hFRCIZ9O09+iI9/kBEJJpBrx69iMgekQz6hhb16EVE2kUy6NWjFxHZI5pB3z5Gr+mVIiLRDPqG9h69bpgSEYlm0De2JMjJMmLZkWyeiEiXRDIJG+JtFMayteiIiAgRDfrGloRm3IiIhNIKejObZmarzGy1md3eyfF+ZvaUmS0xs+VmdnXKsVvMbFm4/0s9Wfn9aWhpo0AzbkREgDSC3syygfuBC4GJwOVmNrFDsRuBt9z9ZOAc4D4zi5nZJODzwFTgZOATZtbpSlQ9qTGuZ9GLiLRLp0c/FVjt7mvcvQWYA0zvUMaBEgsGxYuBbUCCYC3Zv7l7o7sngL8An+qx2u9HQ0ub5tCLiITSCfoRwPqU7apwX6ofE4T6RmApcIu7J4FlwNlmNsjMCoGLgJGdXcTMrjezRWa2qLa2tovN2JvG6EVE9kgn6DubuuIdtj8GLAaGA1OAH5tZqbuvAL4D/BGYDywh6Onv+4HuD7p7pbtXlpWVpVv/TjXG1aMXEWmXTtBXsXcvvIKg557qamCuB1YDa4ETANz9IXf/gLufTTCk8073q31gWi9WRGSPdIL+VWCcmY01sxgwA3iyQ5l1wPkAZlYOjAfWhNtDwp+jgEuA2T1T9f1rjLfprlgRkdBBu73unjCzm4BngGzgYXdfbmY3hMdnAXcBj5rZUlVU4oYAAAjCSURBVIKhnq+6+5bwI/7LzAYBrcCN7r69NxqSUl8aW9vUoxcRCaWVhu4+D5jXYd+slPcbgY/u59yzulPBroonkrQlXT16EZFQ5O6MbX9EsXr0IiKByAW9FgYXEdlb5IJ+d49e8+hFRIAIBn37MoJ61o2ISCByQd8Y1xi9iEiqyAV9e49eY/QiIoHIBX1jGPQaoxcRCUQu6Bt2D92oRy8iAhEM+vYefaF69CIiQASDvr1HX5CrHr2ICEQw6Jta2yjIzSY7SwuDi4hABIO+IZ6gSM+5ERHZLXJB39jSRqHm0IuI7Ba5oG+IJzSHXkQkReSCvlELg4uI7CWtoDezaWa2ysxWm9ntnRzvZ2ZPmdkSM1tuZlenHPtyuG+Zmc02s/yebEBHDVoYXERkLwcNejPLBu4HLgQmApeb2cQOxW4E3nL3k4FzgPvMLGZmI4CbgUp3n0SwQtWMHqz/PrQwuIjI3tLp0U8FVrv7GndvAeYA0zuUcaDEzAwoJlgEPBEeywEKzCwHKGTfhcV7lBYGFxHZWzpBPwJYn7JdFe5L9WNgAkGILwVucfeku28A7iVYPHwTsNPdn+3sImZ2vZktMrNFtbW1XWzGHo0tWhhcRCRVOkHf2Z1H3mH7Y8BiYDgwBfixmZWa2QCC3v/Y8FiRmX22s4u4+4PuXunulWVlZWk3oKOGuHr0IiKp0gn6KmBkynYF+w6/XA3M9cBqYC1wAnABsNbda929FZgLnNH9ancu0ZYknkhqHr2ISIp0gv5VYJyZjTWzGMGXqU92KLMOOB/AzMqB8cCacP9pZlYYjt+fD6zoqcp31Njavoyghm5ERNodtOvr7gkzuwl4hmDWzMPuvtzMbgiPzwLuAh41s6UEQz1fdfctwBYz+x3wOsGXs28AD/ZOU/asLqUevYjIHmklorvPA+Z12Dcr5f1G4KP7OffrwNe7Uce07Vl0RD16EZF2kboztrFFPXoRkY4iFfQNca0XKyLSUaSCfk+PXkEvItIuUkHfoIXBRUT2Eamg3zPrRj16EZF2kQr63T16fRkrIrJbpIJ+9xi9pleKiOwWqaBviCfIyTJi2ZFqlohIt0QqEdtXlwqetiAiIhCxoG+Ia3UpEZGOIhX0Wi9WRGRfEQt69ehFRDqKVNA3qEcvIrKPSAV9Y0tCDzQTEekgWkEfV49eRKSjSAV9Q4vWixUR6SitoDezaWa2ysxWm9ntnRzvZ2ZPmdkSM1tuZleH+8eb2eKUV52ZfamnG9GuMd6mu2JFRDo4aPfXzLKB+4GPECwU/qqZPenub6UUuxF4y93/3szKgFVm9pi7rwKmpHzOBuCJnm5EuwsmljO5ol9vfbyIyFEpnXGOqcBqd18DYGZzgOlAatA7UBIuAF4MbCNYIzbV+cC77v5+t2u9Hz/4X1N666NFRI5a6QzdjADWp2xXhftS/RiYAGwElgK3uHuyQ5kZwOz9XcTMrjezRWa2qLa2No1qiYhIOtIJ+s4eHOMdtj8GLAaGEwzV/NjMSnd/gFkM+CTw2/1dxN0fdPdKd68sKytLo1oiIpKOdIK+ChiZsl1B0HNPdTUw1wOrgbXACSnHLwRed/fq7lRWRES6Lp2gfxUYZ2Zjw575DODJDmXWEYzBY2blwHhgTcrxyznAsI2IiPSeg34Z6+4JM7sJeAbIBh529+VmdkN4fBZwF/ComS0lGOr5qrtvATCzQoIZO1/opTaIiMgBpHV3kbvPA+Z12Dcr5f1G4KP7ObcRGNSNOoqISDdE6s5YERHZl4JeRCTizL3jTMm+Z2a1wKHeWDUY2NKD1TkaZGKbITPbnYlthsxsd1fbPNrdO52bfkQGfXeY2SJ3r+zrehxOmdhmyMx2Z2KbITPb3ZNt1tCNiEjEKehFRCIuikH/YF9XoA9kYpshM9udiW2GzGx3j7U5cmP0IiKytyj26EVEJIWCXkQk4iIT9Adb7jAqzGykmT1vZivCZRtvCfcPNLM/mtk74c8BfV3XnmZm2Wb2hpk9HW5nQpv7m9nvzGxl+Ht+etTbbWZfDv9sLzOz2WaWH8U2m9nDZlZjZstS9u23nWZ2R5hvq8zsY125ViSCPmW5wwuBicDlZjaxb2vVaxLAre4+ATgNuDFs6+3Ac+4+Dngu3I6aW4AVKduZ0OaZwHx3PwE4maD9kW23mY0AbgYq3X0SwYMUZxDNNj8KTOuwr9N2hn/HZwAnhuf8Z5h7aYlE0JOy3KG7twDtyx1GjrtvcvfXw/f1BH/xRxC09+dhsZ8DF/dNDXuHmVUAHwd+lrI76m0uBc4GHgJw9xZ330HE203wsMUCM8sBCgnWv4hcm939BYJlV1Ptr53TgTnuHnf3tcBqgtxLS1SCPp3lDiPHzMYApwCvAOXuvgmCfwyAIX1Xs17xQ+A2IHWJyqi3+RigFngkHLL6mZkVEeF2u/sG4F6CNS42ATvd/Vki3OYO9tfObmVcVII+neUOI8XMioH/Ar7k7nV9XZ/eZGafAGrc/bW+rsthlgN8APiJu58CNBCNIYv9CsekpwNjCZYmLTKzz/ZtrY4I3cq4qAR9OssdRoaZ5RKE/GPuPjfcXW1mw8Ljw4CavqpfLzgT+KSZvUcwLHeemf2KaLcZgj/XVe7+Srj9O4Lgj3K7LwDWunutu7cCc4EziHabU+2vnd3KuKgEfTrLHUaCmRnBmO0Kd/9+yqEngavC91cBfzjcdest7n6Hu1e4+xiC39s/u/tniXCbAdx9M7DezMaHu84H3iLa7V4HnGZmheGf9fMJvoeKcptT7a+dTwIzzCzPzMYC44CFaX+qu0fiBVwEvA28C/xrX9enF9v5dwT/ZXsTWBy+LiJYxes54J3w58C+rmsvtf8c4OnwfeTbDEwBFoW/378HBkS93cA3gJXAMuCXQF4U20ywjvYmoJWgx37tgdoJ/GuYb6uAC7tyLT0CQUQk4qIydCMiIvuhoBcRiTgFvYhIxCnoRUQiTkEvIhJxCnoRkYhT0IuIRNz/B8FLKxNuQII7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "\n",
    "dataset.fit((cac for cac in grouped_data.cac.unique())\n",
    "            ,(product for product in grouped_data.product_code.unique())\n",
    "           )\n",
    "\n",
    "num_users, num_items = dataset.interactions_shape()\n",
    "print('Num users: {}, num_items {}.'.format(num_users, num_items))\n",
    "\n",
    "    \n",
    "feature_combo = ['country_code', 'cost_bin', 'product_code']\n",
    "\n",
    "for product_feature in feature_combo:\n",
    "    dataset.fit_partial(users= (cac for cac in grouped_data.cac.unique())\n",
    "                    , items= (product for product in grouped_data.product_code.unique())\n",
    "                    , item_features= (feature for feature in grouped_data[product_feature].unique())\n",
    "                   )\n",
    "\n",
    "(interactions, weights) = \\\n",
    "    dataset.build_interactions(((a[0], a[1]) for a in grouped_data[['cac', 'product_code']].itertuples(index= False)))\n",
    "\n",
    "item_features = dataset.build_item_features(((getattr(row, 'product_code'), [getattr(row, product_feature) for product_feature in feature_combo if product_feature != 'product_code']) \\\n",
    "         for row in grouped_data[feature_combo].itertuples()))\n",
    "\n",
    "model_loop = LightFM(loss= 'warp',no_components = 25, max_sampled = 5, learning_rate= 0.05, k= 5, n= 20)\n",
    "\n",
    "(train_loop, test_loop) = random_train_test_split(interactions= interactions, test_percentage= 0.2, random_state = 40)\n",
    "\n",
    "warp_duration = []\n",
    "warp_auc = []\n",
    "warp_map_at_15 = []\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Processing epoch: ', epoch)\n",
    "    start = time.time()\n",
    "    model_loop.fit_partial(train_loop, epochs=1, verbose = False)\n",
    "    warp_duration.append(time.time() - start)\n",
    "    warp_auc.append(auc_score(model_loop, test_loop, train_interactions= train_loop).mean())\n",
    "    warp_map_at_15.append(precision_at_k(model_loop, test_loop, train_interactions= train_loop, k= 15).mean())\n",
    "\n",
    "x = np.arange(epochs)\n",
    "plt.plot(x, np.array(warp_duration))\n",
    "plt.legend(['WARP duration'], loc='upper right')\n",
    "plt.title('Duration')\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(epochs)\n",
    "plt.plot(x, np.array(warp_auc))\n",
    "plt.legend(['WARP AUC'], loc='upper right')\n",
    "plt.title('AUC')\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(epochs)\n",
    "plt.plot(x, np.array(warp_map_at_15))\n",
    "plt.legend(['WARP MAP@15'], loc='upper right')\n",
    "plt.title('Mean Average Precision @ K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot clearly shows that 60 epochs maximize both AUC and Precision@15.\n",
    "\n",
    "The final hybrid model is then trained using the parameters:\n",
    "\n",
    "| Parameter | Value |       \n",
    "| :- |:-------------: |\n",
    "|k| 5 | \n",
    "|learning_rate| 0.05 | \n",
    "|n| 20 | \n",
    "|no_components| 25 | \n",
    "|epochs | 60|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Precision: train 0.10, test 0.03.\n",
      "AUC: train 0.82, test 0.82.\n"
     ]
    }
   ],
   "source": [
    "model_loop = LightFM(loss= 'warp',no_components = 25, max_sampled = 5, learning_rate= 0.05, k= 5, n= 20)\n",
    "    \n",
    "features =  ['product_code', 'country_code', 'cost_bin']\n",
    "model_data: pd.DataFrame = grouped_data[features]\n",
    "model_loop = LightFM(loss= 'warp', **params)\n",
    "\n",
    "train_hybrid_model(model_loop, model_data, epochs= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Features Manually\n",
    "\n",
    "After analysing similar issues, the poor performance is often associated with very popular items.\n",
    "\n",
    "Whilst LightFM does normalize data, it does so in its own peculiar way. \n",
    "\n",
    "To improve the model's performance we applied two transformations:\n",
    "- (Optional) Establish a maximum frequency of 5 instead of the current 12\n",
    "- Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data_normalized = grouped_data.copy()\n",
    "grouped_data_normalized['volume_primary_units'] = grouped_data_normalized['volume_primary_units'].apply(\n",
    "                    lambda x: min(x, 50)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n",
      "Epoch 51\n",
      "Epoch 52\n",
      "Epoch 53\n",
      "Epoch 54\n",
      "Epoch 55\n",
      "Epoch 56\n",
      "Epoch 57\n",
      "Epoch 58\n",
      "Epoch 59\n",
      "Precision: train 0.12, test 0.07.\n",
      "AUC: train 0.83, test 0.84.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x20d0b0ab8e0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFM(loss= 'warp',no_components = 25, max_sampled = 5, learning_rate= 0.05, k= 5, n= 20)\n",
    "\n",
    "train_hybrid_model(model, grouped_data_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Limiting an interaction's max strength seems to have increased the test Precision@15 from 0.03 to 0.07.\n",
    "<br><br>\n",
    "To understand if LightFM's normalizer is also generating problems on its own, let us normalize our data manually and feed it into LightFM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>country_code</th>\n",
       "      <th>cac</th>\n",
       "      <th>product_code</th>\n",
       "      <th>invoiced_sales</th>\n",
       "      <th>profit</th>\n",
       "      <th>group</th>\n",
       "      <th>region_group</th>\n",
       "      <th>company_area</th>\n",
       "      <th>product_family</th>\n",
       "      <th>product_subfamily</th>\n",
       "      <th>profit_bin</th>\n",
       "      <th>cost_bin</th>\n",
       "      <th>volume_primary_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>country_code_0</td>\n",
       "      <td>cac_0</td>\n",
       "      <td>product_code_1008</td>\n",
       "      <td>0.016805</td>\n",
       "      <td>0.024820</td>\n",
       "      <td>group_3</td>\n",
       "      <td>region_7</td>\n",
       "      <td>company_area_7</td>\n",
       "      <td>product_family_7</td>\n",
       "      <td>product_subfamily_17</td>\n",
       "      <td>4.032902</td>\n",
       "      <td>4.032902</td>\n",
       "      <td>0.568260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>country_code_0</td>\n",
       "      <td>cac_0</td>\n",
       "      <td>product_code_1049</td>\n",
       "      <td>-0.275560</td>\n",
       "      <td>-0.498534</td>\n",
       "      <td>group_5</td>\n",
       "      <td>region_19</td>\n",
       "      <td>company_area_22</td>\n",
       "      <td>product_family_32</td>\n",
       "      <td>product_subfamily_103</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>0.568260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>country_code_0</td>\n",
       "      <td>cac_0</td>\n",
       "      <td>product_code_1049</td>\n",
       "      <td>-0.275560</td>\n",
       "      <td>-0.298600</td>\n",
       "      <td>group_5</td>\n",
       "      <td>region_19</td>\n",
       "      <td>company_area_22</td>\n",
       "      <td>product_family_32</td>\n",
       "      <td>product_subfamily_103</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>0.568260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>country_code_0</td>\n",
       "      <td>cac_0</td>\n",
       "      <td>product_code_1060</td>\n",
       "      <td>-0.076492</td>\n",
       "      <td>-0.056984</td>\n",
       "      <td>group_3</td>\n",
       "      <td>region_5</td>\n",
       "      <td>company_area_5</td>\n",
       "      <td>product_family_5</td>\n",
       "      <td>product_subfamily_6</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>0.568260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>country_code_0</td>\n",
       "      <td>cac_0</td>\n",
       "      <td>product_code_1124</td>\n",
       "      <td>-0.159166</td>\n",
       "      <td>-0.147322</td>\n",
       "      <td>group_3</td>\n",
       "      <td>region_7</td>\n",
       "      <td>company_area_7</td>\n",
       "      <td>product_family_7</td>\n",
       "      <td>product_subfamily_24</td>\n",
       "      <td>1.929193</td>\n",
       "      <td>1.929193</td>\n",
       "      <td>0.568260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885824</th>\n",
       "      <td>2951293</td>\n",
       "      <td>country_code_3</td>\n",
       "      <td>cac_9999</td>\n",
       "      <td>product_code_3417</td>\n",
       "      <td>-0.132061</td>\n",
       "      <td>-0.088422</td>\n",
       "      <td>group_3</td>\n",
       "      <td>region_16</td>\n",
       "      <td>company_area_19</td>\n",
       "      <td>product_family_22</td>\n",
       "      <td>product_subfamily_17</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-1.759758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885825</th>\n",
       "      <td>2951294</td>\n",
       "      <td>country_code_3</td>\n",
       "      <td>cac_9999</td>\n",
       "      <td>product_code_3456</td>\n",
       "      <td>-0.132061</td>\n",
       "      <td>-0.091443</td>\n",
       "      <td>group_6</td>\n",
       "      <td>region_20</td>\n",
       "      <td>company_area_23</td>\n",
       "      <td>product_family_48</td>\n",
       "      <td>product_subfamily_193</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-1.759758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885826</th>\n",
       "      <td>2951295</td>\n",
       "      <td>country_code_3</td>\n",
       "      <td>cac_9999</td>\n",
       "      <td>product_code_3594</td>\n",
       "      <td>0.439170</td>\n",
       "      <td>0.218382</td>\n",
       "      <td>group_3</td>\n",
       "      <td>region_6</td>\n",
       "      <td>company_area_6</td>\n",
       "      <td>product_family_6</td>\n",
       "      <td>product_subfamily_76</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-1.759758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885827</th>\n",
       "      <td>2951296</td>\n",
       "      <td>country_code_3</td>\n",
       "      <td>cac_9999</td>\n",
       "      <td>product_code_4477</td>\n",
       "      <td>-0.178838</td>\n",
       "      <td>-0.145897</td>\n",
       "      <td>group_5</td>\n",
       "      <td>region_15</td>\n",
       "      <td>company_area_18</td>\n",
       "      <td>product_family_30</td>\n",
       "      <td>product_subfamily_61</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-0.174517</td>\n",
       "      <td>-1.759758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885828</th>\n",
       "      <td>2951297</td>\n",
       "      <td>country_code_3</td>\n",
       "      <td>cac_9999</td>\n",
       "      <td>product_code_697</td>\n",
       "      <td>0.118042</td>\n",
       "      <td>0.168448</td>\n",
       "      <td>group_3</td>\n",
       "      <td>region_13</td>\n",
       "      <td>company_area_13</td>\n",
       "      <td>product_family_15</td>\n",
       "      <td>product_subfamily_32</td>\n",
       "      <td>1.929193</td>\n",
       "      <td>1.929193</td>\n",
       "      <td>-1.759758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2885829 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index    country_code       cac       product_code  invoiced_sales  \\\n",
       "0              0  country_code_0     cac_0  product_code_1008        0.016805   \n",
       "1              1  country_code_0     cac_0  product_code_1049       -0.275560   \n",
       "2              2  country_code_0     cac_0  product_code_1049       -0.275560   \n",
       "3              4  country_code_0     cac_0  product_code_1060       -0.076492   \n",
       "4              5  country_code_0     cac_0  product_code_1124       -0.159166   \n",
       "...          ...             ...       ...                ...             ...   \n",
       "2885824  2951293  country_code_3  cac_9999  product_code_3417       -0.132061   \n",
       "2885825  2951294  country_code_3  cac_9999  product_code_3456       -0.132061   \n",
       "2885826  2951295  country_code_3  cac_9999  product_code_3594        0.439170   \n",
       "2885827  2951296  country_code_3  cac_9999  product_code_4477       -0.178838   \n",
       "2885828  2951297  country_code_3  cac_9999   product_code_697        0.118042   \n",
       "\n",
       "           profit    group region_group     company_area     product_family  \\\n",
       "0        0.024820  group_3     region_7   company_area_7   product_family_7   \n",
       "1       -0.498534  group_5    region_19  company_area_22  product_family_32   \n",
       "2       -0.298600  group_5    region_19  company_area_22  product_family_32   \n",
       "3       -0.056984  group_3     region_5   company_area_5   product_family_5   \n",
       "4       -0.147322  group_3     region_7   company_area_7   product_family_7   \n",
       "...           ...      ...          ...              ...                ...   \n",
       "2885824 -0.088422  group_3    region_16  company_area_19  product_family_22   \n",
       "2885825 -0.091443  group_6    region_20  company_area_23  product_family_48   \n",
       "2885826  0.218382  group_3     region_6   company_area_6   product_family_6   \n",
       "2885827 -0.145897  group_5    region_15  company_area_18  product_family_30   \n",
       "2885828  0.168448  group_3    region_13  company_area_13  product_family_15   \n",
       "\n",
       "             product_subfamily  profit_bin  cost_bin  volume_primary_units  \n",
       "0         product_subfamily_17    4.032902  4.032902              0.568260  \n",
       "1        product_subfamily_103   -0.174517 -0.174517              0.568260  \n",
       "2        product_subfamily_103   -0.174517 -0.174517              0.568260  \n",
       "3          product_subfamily_6   -0.174517 -0.174517              0.568260  \n",
       "4         product_subfamily_24    1.929193  1.929193              0.568260  \n",
       "...                        ...         ...       ...                   ...  \n",
       "2885824   product_subfamily_17   -0.174517 -0.174517             -1.759758  \n",
       "2885825  product_subfamily_193   -0.174517 -0.174517             -1.759758  \n",
       "2885826   product_subfamily_76   -0.174517 -0.174517             -1.759758  \n",
       "2885827   product_subfamily_61   -0.174517 -0.174517             -1.759758  \n",
       "2885828   product_subfamily_32    1.929193  1.929193             -1.759758  \n",
       "\n",
       "[2885829 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data_normalized = grouped_data.copy()\n",
    "\n",
    "grouped_data_normalized['volume_primary_units'] = grouped_data_normalized['volume_primary_units'].apply(\n",
    "                    lambda x: min(x, 2)\n",
    ")\n",
    "\n",
    "# Select only numeric variables\n",
    "num_cols = grouped_data_normalized.columns[grouped_data_normalized.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "\n",
    "x = grouped_data_normalized[num_cols].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "grouped_data_normalized = grouped_data_normalized.reset_index()\n",
    "grouped_data_normalized[num_cols] = pd.DataFrame(x_scaled)\n",
    "\n",
    "grouped_data_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model on both: manually standardized features and with popularity limitation = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n",
      "Epoch 51\n",
      "Epoch 52\n",
      "Epoch 53\n",
      "Epoch 54\n",
      "Epoch 55\n",
      "Epoch 56\n",
      "Epoch 57\n",
      "Epoch 58\n",
      "Epoch 59\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision@15: train 0.03, test 0.01.\n",
      "AUC@15: train 0.38, test 0.37.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x23a665c0310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFM(loss= 'warp', no_components = 25, max_sampled = 5, learning_rate= 0.05, k= 5, n= 20)\n",
    "\n",
    "train_hybrid_model(model, grouped_data_normalized, normalize_features= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through feature selection to retrieve the best performing feature combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product_code', 'country_code']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision: train 0.11, test 0.07.\n",
      "AUC: train 0.83, test 0.84.\n",
      "['product_code', 'country_code', 'cost_bin']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision: train 0.11, test 0.06.\n",
      "AUC: train 0.83, test 0.84.\n",
      "['product_code', 'country_code', 'cost_bin', 'group']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision: train 0.11, test 0.07.\n",
      "AUC: train 0.83, test 0.84.\n",
      "['product_code', 'country_code', 'cost_bin', 'group', 'region_group']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision: train 0.11, test 0.06.\n",
      "AUC: train 0.83, test 0.84.\n",
      "['product_code', 'country_code', 'cost_bin', 'group', 'region_group', 'company_area']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision: train 0.11, test 0.07.\n",
      "AUC: train 0.83, test 0.84.\n",
      "['product_code', 'country_code', 'cost_bin', 'group', 'region_group', 'company_area', 'product_family']\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision: train 0.11, test 0.07.\n",
      "AUC: train 0.83, test 0.84.\n"
     ]
    }
   ],
   "source": [
    "feature_combinations = [\n",
    "    ['product_code', 'country_code', 'cost_bin', 'group', 'region_group',\n",
    "                                              'company_area', 'product_family'],\n",
    "    ['product_code', 'country_code', 'cost_bin', 'group', 'region_group',\n",
    "                                              'company_area'],\n",
    "    ['product_code', 'country_code', 'cost_bin', 'group', 'region_group'],\n",
    "    ['product_code', 'country_code', 'cost_bin', 'group'],\n",
    "    ['product_code', 'country_code', 'cost_bin'],\n",
    "    ['product_code', 'country_code']\n",
    "    \n",
    "]\n",
    "\n",
    "feature_combinations.reverse()\n",
    "\n",
    "for feature_combo in feature_combinations:\n",
    "    print(feature_combo)\n",
    "    \n",
    "    model_data: pd.DataFrame = grouped_data[feature_combo]\n",
    "    model_loop = LightFM(loss= 'warp', no_components = 25, max_sampled = 5)\n",
    "\n",
    "    train_hybrid_model(model_loop, grouped_data_normalized, epochs= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n",
      "Epoch 51\n",
      "Epoch 52\n",
      "Epoch 53\n",
      "Epoch 54\n",
      "Epoch 55\n",
      "Epoch 56\n",
      "Epoch 57\n",
      "Epoch 58\n",
      "Epoch 59\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision@15: train 0.13, test 0.07.\n",
      "AUC@15: train 0.84, test 0.85.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x23a665c0580>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFM(loss= 'warp', no_components = 100, max_sampled = 5, learning_rate= 0.05, k= 5, n= 20)\n",
    "\n",
    "train_hybrid_model(model, grouped_data_normalized, epochs= 60)\n",
    "\n",
    "with open('hybrid_model_150.pickle', 'wb') as pickle_out:\n",
    "    pickle.dump(model, pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simpler model using only a capped interaction strength limited at 50 interaactions seems to yield the best result with the lowest complexity. For this reason, we considered this to be the final and best performing hybrid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n",
      "Epoch 51\n",
      "Epoch 52\n",
      "Epoch 53\n",
      "Epoch 54\n",
      "Epoch 55\n",
      "Epoch 56\n",
      "Epoch 57\n",
      "Epoch 58\n",
      "Epoch 59\n",
      "Num users: 8100, num_items 23170.\n",
      "Precision@15: train 0.11, test 0.06.\n",
      "AUC@15: train 0.83, test 0.84.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x23121fd58b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data_normalized = grouped_data.copy()\n",
    "grouped_data_normalized['volume_primary_units'] = grouped_data_normalized['volume_primary_units'].apply(\n",
    "                    lambda x: min(x, 50)\n",
    "            )\n",
    "\n",
    "model = LightFM(loss= 'warp',no_components = 25, max_sampled = 5, learning_rate= 0.05, k= 5, n= 20)\n",
    "\n",
    "train_hybrid_model(model, grouped_data_normalized)\n",
    "\n",
    "with open('hybrid_model.pickle', 'wb') as pickle_out:\n",
    "    pickle.dump(model, pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model - Profitability\n",
    "\n",
    "Estimated profitability at the different precision levels can be computed by retrieving each customer's precision_at_k, and using it as a multiplier on the customer's test dataset. LightFM's precision_at_k returns each user's Precision@k (which is actually a MAP@K in LightFM), which then enables calculating each user's sales in the test set and multiply by the obtained Precision@k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to maintain comparability between models the same method has to be used. We have then used each Precision@k as a multiplier on the test set, shuffling the data 50 times and averaging its result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n",
      "Epoch 51\n",
      "Epoch 52\n",
      "Epoch 53\n",
      "Epoch 54\n",
      "Epoch 55\n",
      "Epoch 56\n",
      "Epoch 57\n",
      "Epoch 58\n",
      "Epoch 59\n",
      "Average Profit at 1:  34547.060662955046\n",
      "Average Profit at 5:  28873.83781939745\n",
      "Average Profit at 10:  25915.815791264176\n",
      "Average Profit at 15:  23447.11723845452\n",
      "Precision@1:  0.1004815399646759\n",
      "Precision@5:  0.0839807391166687\n",
      "Precision@10:  0.0753772109746933\n",
      "Precision@15:  0.06819690018892288\n",
      "Recall@1:  0.0011973599856129865\n",
      "Recall@5:  0.006955157520500542\n",
      "Recall@10:  0.013616062545696181\n",
      "Recall@15:  0.020179178717393755\n",
      "AUC:  0.8406723\n",
      "AUC@1:  1.0\n",
      "AUC@5:  0.23\n",
      "AUC@10:  0.2\n",
      "AUC@15:  0.14\n"
     ]
    }
   ],
   "source": [
    "# Train a model manually so the train and test datasets can be accessed on a need-basis\n",
    "grouped_data_normalized = grouped_data.copy()\n",
    "grouped_data_normalized['volume_primary_units'] = grouped_data_normalized['volume_primary_units'].apply(\n",
    "                    lambda x: min(x, 50)\n",
    "            )\n",
    "\n",
    "dataset = Dataset()\n",
    "data = grouped_data_normalized\n",
    "\n",
    "dataset.fit((cac for cac in data.cac.unique())\n",
    "            ,(product for product in data.product_code.unique())\n",
    "           )\n",
    "\n",
    "features =  ['product_code', 'country_code', 'cost_bin']\n",
    "\n",
    "for product_feature in features:\n",
    "    dataset.fit_partial(users= (cac for cac in data.cac.unique())\n",
    "                    , items= (product for product in data.product_code.unique())\n",
    "                    , item_features= (feature for feature in data[product_feature].unique())\n",
    "                   )\n",
    "\n",
    "(interactions, weights) = \\\n",
    "    dataset.build_interactions(((a[0], a[1]) for a in data[['cac', 'product_code']].itertuples(index= False)))\n",
    "\n",
    "item_features = dataset.build_item_features(((getattr(row, 'product_code'), [getattr(row, product_feature) for features in features if product_feature != 'product_code']) \\\n",
    "         for row in data[features].itertuples()), normalize = True)\n",
    "\n",
    "model_loop = LightFM(loss= 'warp',no_components = 25, max_sampled = 5, learning_rate= 0.05, k= 5, n= 20)\n",
    "\n",
    "(train_loop, test_loop) = random_train_test_split(interactions= interactions, test_percentage= 0.2, random_state = 40)\n",
    "model_loop.fit(train_loop, item_features= item_features, epochs= 60, verbose = True)\n",
    "\n",
    "metric_params = {\n",
    "    'model': model_loop,\n",
    "    'test_interactions': test_loop,\n",
    "    'item_features': item_features\n",
    "}\n",
    "\n",
    "precision_at_1 = lightfm.evaluation.precision_at_k(**metric_params, k=1)\n",
    "precision_at_5 = lightfm.evaluation.precision_at_k(**metric_params, k=5)\n",
    "precision_at_10 = lightfm.evaluation.precision_at_k(**metric_params, k=10)\n",
    "precision_at_15 = lightfm.evaluation.precision_at_k(**metric_params, k=15)\n",
    "\n",
    "recall_at_1 = lightfm.evaluation.recall_at_k(**metric_params, k=1)\n",
    "recall_at_5 = lightfm.evaluation.recall_at_k(**metric_params, k=5)\n",
    "recall_at_10 = lightfm.evaluation.recall_at_k(**metric_params, k=10)\n",
    "recall_at_15 = lightfm.evaluation.recall_at_k(**metric_params, k=15)\n",
    "\n",
    "auc = lightfm.evaluation.auc_score(**metric_params).mean()\n",
    "\n",
    "auc_at_1 = hybrid_auc_at_k(precision_at_1, recall_at_1)\n",
    "auc_at_5 = hybrid_auc_at_k(precision_at_5, recall_at_5)\n",
    "auc_at_10 = hybrid_auc_at_k(precision_at_10, recall_at_10)\n",
    "auc_at_15 = hybrid_auc_at_k(precision_at_15, recall_at_15)\n",
    "\n",
    "precision_at_1 = precision_at_1.mean().item()\n",
    "precision_at_5 = precision_at_5.mean().item()\n",
    "precision_at_10 = precision_at_10.mean().item()\n",
    "precision_at_15 = precision_at_15.mean().item()\n",
    "\n",
    "recall_at_1 = recall_at_1.mean().item()\n",
    "recall_at_5 = recall_at_5.mean().item()\n",
    "recall_at_10 = recall_at_10.mean().item()\n",
    "recall_at_15 = recall_at_15.mean().item()\n",
    "\n",
    "avg_profit_at_1: float = 0.0\n",
    "avg_profit_at_5: float = 0.0\n",
    "avg_profit_at_10: float = 0.0\n",
    "avg_profit_at_15: float = 0.0\n",
    "    \n",
    "shuffle_count: int = 50\n",
    "\n",
    "for i in range(shuffle_count):   \n",
    "    indices = np.arange(test_loop.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_matrix = test_loop.T.tocsr()[list(indices)] \n",
    "    \n",
    "    avg_profit_at_1 = avg_profit_at_1 + (shuffled_matrix * precision_at_1).sum()\n",
    "    avg_profit_at_5 = avg_profit_at_5 + (shuffled_matrix * precision_at_5).sum()\n",
    "    avg_profit_at_10 = avg_profit_at_10 + (shuffled_matrix * precision_at_10).sum()\n",
    "    avg_profit_at_15 = avg_profit_at_15 + (shuffled_matrix * precision_at_15).sum()\n",
    "    \n",
    "avg_profit_at_1 = avg_profit_at_1 / shuffle_count\n",
    "avg_profit_at_5 = avg_profit_at_5 / shuffle_count\n",
    "avg_profit_at_10 = avg_profit_at_10 / shuffle_count\n",
    "avg_profit_at_15 = avg_profit_at_15 / shuffle_count\n",
    "\n",
    "print('Average Profit at 1: ', avg_profit_at_1)\n",
    "print('Average Profit at 5: ', avg_profit_at_5)\n",
    "print('Average Profit at 10: ', avg_profit_at_10)\n",
    "print('Average Profit at 15: ', avg_profit_at_15)\n",
    "\n",
    "print('Precision@1: ', precision_at_1)\n",
    "print('Precision@5: ', precision_at_5)\n",
    "print('Precision@10: ', precision_at_10)\n",
    "print('Precision@15: ', precision_at_15)\n",
    "\n",
    "print('Recall@1: ', recall_at_1)\n",
    "print('Recall@5: ', recall_at_5)\n",
    "print('Recall@10: ', recall_at_10)\n",
    "print('Recall@15: ', recall_at_15)\n",
    "\n",
    "print('AUC: ', auc)\n",
    "print('AUC@1: ', auc_at_1)\n",
    "print('AUC@5: ', auc_at_5)\n",
    "print('AUC@10: ', auc_at_10)\n",
    "print('AUC@15: ', auc_at_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profitability outcome is greatly influenced by the train test split, and might differ drastically from other model's. Hence, to mitigate such differences, we generate random train test splits and use them to measure profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-594f76c31cae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mmodel_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLightFM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'warp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mno_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_sampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_train_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteractions\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0minteractions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_percentage\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mmodel_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightfm\\cross_validation.py\u001b[0m in \u001b[0;36mrandom_train_test_split\u001b[1;34m(interactions, test_percentage, random_state)\u001b[0m\n\u001b[0;32m     62\u001b[0m                         interactions.data)\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0muids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtest_percentage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightfm\\cross_validation.py\u001b[0m in \u001b[0;36m_shuffle\u001b[1;34m(uids, iids, data, random_state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     return (uids[shuffle_indices],\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "# Train a model manually so the train and test datasets can be accessed on a need-basis\n",
    "grouped_data_normalized = grouped_data.copy()\n",
    "grouped_data_normalized['volume_primary_units'] = grouped_data_normalized['volume_primary_units'].apply(\n",
    "                    lambda x: min(x, 50)\n",
    "            )\n",
    "\n",
    "dataset = Dataset()\n",
    "data = grouped_data_normalized\n",
    "\n",
    "dataset.fit((cac for cac in data.cac.unique())\n",
    "            ,(product for product in data.product_code.unique())\n",
    "           )\n",
    "\n",
    "features =  ['product_code', 'country_code', 'cost_bin']\n",
    "\n",
    "for product_feature in features:\n",
    "    dataset.fit_partial(users= (cac for cac in data.cac.unique())\n",
    "                    , items= (product for product in data.product_code.unique())\n",
    "                    , item_features= (feature for feature in data[product_feature].unique())\n",
    "                   )\n",
    "\n",
    "(interactions, weights) = \\\n",
    "    dataset.build_interactions(((a[0], a[1]) for a in data[['cac', 'product_code']].itertuples(index= False)))\n",
    "\n",
    "item_features = dataset.build_item_features(((getattr(row, 'product_code'), [getattr(row, product_feature) for features in features if product_feature != 'product_code']) \\\n",
    "         for row in data[features].itertuples()), normalize = True)\n",
    "\n",
    "model_loop = LightFM(loss= 'warp',no_components = 25, max_sampled = 5, learning_rate= 0.05, k= 5, n= 20)\n",
    "\n",
    "(train_loop, test_loop) = random_train_test_split(interactions= interactions, test_percentage= 0.2, random_state = 40)\n",
    "model_loop.fit(train_loop, item_features= item_features, epochs= 60, verbose = True)\n",
    "\n",
    "metric_params = {\n",
    "    'model': model_loop,\n",
    "    'test_interactions': test_loop,\n",
    "    'item_features': item_features\n",
    "}\n",
    "\n",
    "precision_at_1 = lightfm.evaluation.precision_at_k(**metric_params, k=1)\n",
    "precision_at_5 = lightfm.evaluation.precision_at_k(**metric_params, k=5)\n",
    "precision_at_10 = lightfm.evaluation.precision_at_k(**metric_params, k=10)\n",
    "precision_at_15 = lightfm.evaluation.precision_at_k(**metric_params, k=15)\n",
    "\n",
    "recall_at_1 = lightfm.evaluation.recall_at_k(**metric_params, k=1)\n",
    "recall_at_5 = lightfm.evaluation.recall_at_k(**metric_params, k=5)\n",
    "recall_at_10 = lightfm.evaluation.recall_at_k(**metric_params, k=10)\n",
    "recall_at_15 = lightfm.evaluation.recall_at_k(**metric_params, k=15)\n",
    "\n",
    "auc = lightfm.evaluation.auc_score(**metric_params).mean()\n",
    "\n",
    "auc_at_1 = hybrid_auc_at_k(precision_at_1, recall_at_1)\n",
    "auc_at_5 = hybrid_auc_at_k(precision_at_5, recall_at_5)\n",
    "auc_at_10 = hybrid_auc_at_k(precision_at_10, recall_at_10)\n",
    "auc_at_15 = hybrid_auc_at_k(precision_at_15, recall_at_15)\n",
    "\n",
    "precision_at_1 = precision_at_1.mean().item()\n",
    "precision_at_5 = precision_at_5.mean().item()\n",
    "precision_at_10 = precision_at_10.mean().item()\n",
    "precision_at_15 = precision_at_15.mean().item()\n",
    "\n",
    "recall_at_1 = recall_at_1.mean().item()\n",
    "recall_at_5 = recall_at_5.mean().item()\n",
    "recall_at_10 = recall_at_10.mean().item()\n",
    "recall_at_15 = recall_at_15.mean().item()\n",
    "\n",
    "avg_profit_at_1: float = 0.0\n",
    "avg_profit_at_5: float = 0.0\n",
    "avg_profit_at_10: float = 0.0\n",
    "avg_profit_at_15: float = 0.0\n",
    "    \n",
    "shuffle_count: int = 50\n",
    "test_split, train_split = np.split(grouped_data['invoiced_sales'], [int(0.2 * len(grouped_data))])\n",
    "\n",
    "for i in range(shuffle_count):   \n",
    "    test_split = test_split.sample(frac=1)\n",
    "    \n",
    "    avg_profit_at_1 = avg_profit_at_1 + (test_split * precision_at_1).sum()\n",
    "    avg_profit_at_5 = avg_profit_at_5 + (test_split * precision_at_5).sum()\n",
    "    avg_profit_at_10 = avg_profit_at_10 + (test_split * precision_at_10).sum()\n",
    "    avg_profit_at_15 = avg_profit_at_15 + (test_split * precision_at_15).sum()\n",
    "    \n",
    "avg_profit_at_1 = avg_profit_at_1 / shuffle_count\n",
    "avg_profit_at_5 = avg_profit_at_5 / shuffle_count\n",
    "avg_profit_at_10 = avg_profit_at_10 / shuffle_count\n",
    "avg_profit_at_15 = avg_profit_at_15 / shuffle_count\n",
    "\n",
    "print('Average Profit at 1: ', avg_profit_at_1)\n",
    "print('Average Profit at 5: ', avg_profit_at_5)\n",
    "print('Average Profit at 10: ', avg_profit_at_10)\n",
    "print('Average Profit at 15: ', avg_profit_at_15)\n",
    "\n",
    "print('Precision@1: ', precision_at_1)\n",
    "print('Precision@5: ', precision_at_5)\n",
    "print('Precision@10: ', precision_at_10)\n",
    "print('Precision@15: ', precision_at_15)\n",
    "\n",
    "print('Recall@1: ', recall_at_1)\n",
    "print('Recall@5: ', recall_at_5)\n",
    "print('Recall@10: ', recall_at_10)\n",
    "print('Recall@15: ', recall_at_15)\n",
    "\n",
    "print('AUC: ', auc)\n",
    "print('AUC@1: ', auc_at_1)\n",
    "print('AUC@5: ', auc_at_5)\n",
    "print('AUC@10: ', auc_at_10)\n",
    "print('AUC@15: ', auc_at_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "The previous table summarizes each model's main findings and results. The output clearly shows the hybrid model yielded the best result and as such, it has been declared the top-performing model.\n",
    "\n",
    "<br>\n",
    "\n",
    "| Metric | Popularity Model | Collaborative Filtering | Hybrid Model |       \n",
    "| :- |:-------------: | :-------------: |:-------------: |\n",
    "|AUC@1| 0 | 0.51 | 1 |\n",
    "|AUC@5| 0.01| 0.54 | 0.23 |\n",
    "|AUC@10| 0 | 0.57 |  0.20|\n",
    "|AUC@15| 0.03 | 0.58|  0.14 |\n",
    "|MAP@1| 0.01 | 0.002 |  0.11|\n",
    "|MAP@5| 0.01 | 0.0009 | 0.08 |\n",
    "|MAP@10| 0 | 0.0006 | 0.07 |\n",
    "|MAP@15| 0 | 0.00005 | 0.07 |\n",
    "|Profitability@1| 7 168 383.20 | 592 829.50 | 36 314 865.61 |\n",
    "|Profitability@5| 5 017 868.25 | 297 798.02 | 30 524 726.26 |\n",
    "|Profitability@10| 4 181 556.87 | 196 579.44 | 26 449 139.05 |\n",
    "|Profitability@15| 4 539 976.03 | 155 794.40 | 24 060 004.71 |\n",
    "\n",
    "\n",
    "### Recommendations Example\n",
    "\n",
    "To showcase the model's output, we provide a set of sample recommendations for a group of users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  15\n",
      "Known items sample: \n",
      "['product_code_1016' 'product_code_1016' 'product_code_1016'\n",
      " 'product_code_1016' 'product_code_1016']\n",
      "Recommended items: \n",
      "[['product_code_1124']\n",
      " ['product_code_1008']\n",
      " ['product_code_1049']\n",
      " ['product_code_1363']\n",
      " ['product_code_1230']\n",
      " ['product_code_1060']\n",
      " ['product_code_14005']\n",
      " ['product_code_13657']\n",
      " ['product_code_1280']\n",
      " ['product_code_12710']\n",
      " ['product_code_11568']\n",
      " ['product_code_13674']\n",
      " ['product_code_14091']\n",
      " ['product_code_12393']\n",
      " ['product_code_12451']]\n",
      "---------\n",
      "User:  20\n",
      "Known items sample: \n",
      "['product_code_2217' 'product_code_2512' 'product_code_364']\n",
      "Recommended items: \n",
      "[['product_code_1124']\n",
      " ['product_code_1008']\n",
      " ['product_code_1049']\n",
      " ['product_code_1363']\n",
      " ['product_code_1230']\n",
      " ['product_code_1060']\n",
      " ['product_code_14005']\n",
      " ['product_code_13657']\n",
      " ['product_code_1280']\n",
      " ['product_code_12710']\n",
      " ['product_code_11568']\n",
      " ['product_code_13674']\n",
      " ['product_code_14091']\n",
      " ['product_code_12393']\n",
      " ['product_code_12451']]\n",
      "---------\n",
      "User:  1200\n",
      "Known items sample: \n",
      "['product_code_107' 'product_code_1079' 'product_code_1419'\n",
      " 'product_code_2793' 'product_code_2793']\n",
      "Recommended items: \n",
      "[['product_code_1124']\n",
      " ['product_code_1008']\n",
      " ['product_code_1049']\n",
      " ['product_code_1363']\n",
      " ['product_code_1230']\n",
      " ['product_code_1060']\n",
      " ['product_code_14005']\n",
      " ['product_code_13657']\n",
      " ['product_code_1280']\n",
      " ['product_code_12710']\n",
      " ['product_code_11568']\n",
      " ['product_code_13674']\n",
      " ['product_code_14091']\n",
      " ['product_code_12393']\n",
      " ['product_code_12451']]\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "user_id = 20\n",
    "recommendations_num = 15\n",
    "recommend_items_hybrid(model, dataset, [15, 20, 1200], 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We have trained 4 models:\n",
    "- Popularity Model\n",
    "- Item Similarity Model\n",
    "- Content Based Filtering\n",
    "- Hybrid Model\n",
    "\n",
    "As expected, the hybrid model outperformed its counterparts, acheiving the best results regarding MAP@K, the reference metric, accompanied by the highest potential profitabiltiy. The reasoning behind the importance of said metric, is that it measures the model's capabilities to suggest items that user actually interacted with, considering the ranked subset of recommendations, given a user cannot cycle through thousands of item recommendations.\n",
    "\n",
    "The LightFM library, even though outperforming its counterparts, still fell short of our expectations. The justification for such defraudation between expectations and reality, might be caused by a lack of proper understanding of the LightFM library, as noted in tickets <a href='https://github.com/lyst/lightfm/issues/430'>430</a>, <a href='https://github.com/lyst/lightfm/issues/486'>486</a>, <a href='https://github.com/lyst/lightfm/issues/497'>497</a> and <a href='https://github.com/lyst/lightfm/issues/551'>551</a>, in particular, the need to handle and weigh highly popular items differently.\n",
    "\n",
    "\n",
    "The best performing model, the hybrid approach, otained a Precision@1 of 0.11 and a potential profitability@15 of 24 060 004.71 monetary units, up to 36 314 865.61 monetary units when considering Precision@15."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
